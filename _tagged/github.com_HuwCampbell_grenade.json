{
	"_id": "14411067",
	"site": "https://github.com/HuwCampbell/grenade",
	"title": "Grenade: Deep Learning in Haskell",
	"author": "vonnik",
	"date": "2017-06-13T13:56:42.055Z",
	"tags": {
		"categories": [
			"opensource",
			"machine-learning",
			"deep-neural-networks",
			"haskell",
			"deep-learning",
			"generative-adversarial-networks",
			"convolutional-neural-networks"
		],
		"languages": [
			"haskell",
			"c",
			"shell"
		]
	},
	"content": "readme.md grenade shalt thou out holy pin, shalt thou count three, more, less.three shall the number thou shalt count, the number the counting shall three.four shalt thou count, neither count thou , excepting thou proceed three.five is right . machine learning might blow in face grenade is composable, dependently typed, practical, fast recurrent neural network library concise precise specifications complex networks haskell. an example, network can achieve ~1.5% error mnist bespecified initialised random weights a few lines code type mnist = network '[ convolution 1 10 5 5 1 1, pooling 2 2 2 2, relu , convolution 10 16 5 5 1 1, pooling 2 2 2 2, flattenlayer, relu , fullyconnected 256 80, logit, fullyconnected 80 10, logit] '[ 'd2 28 28, 'd3 24 24 10, 'd3 12 12 10, 'd3 12 12 10 , 'd3 8 8 16, 'd3 4 4 16, 'd1 256, 'd1 256 , 'd1 80, 'd1 80, 'd1 10, 'd1 10]randommnist :: monadrandom m => m mnistrandommnist = randomnetwork that's . because types are rich, 's specific term level coderequired construct network; although is course possible easy construct deconstruct networks layers explicitly oneself. recurrent neural networks are more style, can try defining something\"unreasonably effective\"type shakespeare = recurrentnetwork '[ r (lstm 40 80), r (lstm 80 40), f (fullyconnected 40 40), f logit] '[ 'd1 40, 'd1 80, 'd1 40, 'd1 40, 'd1 40 ]designnetworks grenade be thought as heterogeneous lists layers, where type includes only layers the network, also shapes data are passed between layers. definition a network is surprisingly simple:data network :: [*] -> [shape] -> * where nnil :: singi => network '[] '[] (:~>) :: (singi , singi h, layer x h) => !x -> !(network xs (h ': hs)) -> network (x ': xs) ( ': h ': hs) layer x o constraint ensures the layer x sensibly perform transformation between input output shapes and o. lifted data kind shape defines 1, 2, 3 dimension types, used declare shape data is passed between layers. the mnist example above, input layer be seen be two dimensional(d2), image 28 28 pixels. the convolution layer runs, outputs three dimensional (d3) 24x24x10 image. last item the list is dimensional (d1) 10 values, representing categories the mnistdata.usage perform propagation, can call eponymous functionbackpropagate :: forall shapes layers. network layers shapes -> s (head shapes) -> s (last shapes) -> gradients layers takes network, appropriate input target data, returns back propagated gradients the network. shapes the gradients areappropriate each layer, may trivial layers relu have learnable parameters. gradients however always applied, yielding new (hopefully better)layer applyupdate :: learningparameters -> network ls ss -> gradients ls -> network ls sslayers grenade are represented haskell classes, creating 's own iseasy downstream code. the shapes a network are specified correctly a layer not sensibly perform operation between shapes, it result a compile error.compositionnetworks layers grenade are easily composed the type level. a networkis instance layer, can a trained network a small component alarger network easily. furthermore, provide 2 layers are designed runlayers parallel merge output (either concatenating across dimension summing pointwise adding activations). allows towrite network can expressed aseries parallel graph. residual network layer specification instance be written type residual net = merge trivial net the type net is instance layer, residual net be too. willrun network, while retaining input passing through trivial layer, merge original image the output. the mnistexample, has been overengineered contain both residual style learning well inception style convolutions.generative adversarial networks grenade is purely functional, can compose training functions flexibleways. gan-mnistexample displays interesting, type safe of writing generative adversarialtraining function 10 lines code.layer zoogrenade layers are normal haskell data types are instance layer, it's easy build 's own downstream code. do however provide decent set layers, including convolution, deconvolution, pooling, pad, crop, logit, relu,elu, tanh, fully connected.build instructionsgrenade is easily built the mafiascript is located the repository. will need lapack blas libraries development tools. once have that, grenade bebuild using:./mafia build the tests run using:./mafia testgrenade builds ghc 7.10 8.0.thankswriting library this has been my mind a while , but big shout must to justin le, whosedependently typed fully connected networkinspired to cracking, gave many ideas the type level tools needed, was great starting point writing library.performancegrenade is backed hmatrix, blas, lapack, critical functions optimised c. using im2col trick popularised caffe, should sufficient many problems.being purely functional, should be easy run batches parallel, would appropriate larger networks, current examples however are singlethreaded.training 15 generations kaggle's 41000 sample mnist training set a singlecore took around 12 minutes, achieving 1.5% error rate a 1000 sample holdout set.contributingcontributions are welcome."
}