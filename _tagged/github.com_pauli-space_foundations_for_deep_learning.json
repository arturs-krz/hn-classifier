{
	"_id": "14484713",
	"site": "https://github.com/pauli-space/foundations_for_deep_learning",
	"title": "Foundations of deep learning",
	"author": "aidanrocke",
	"date": "2017-06-13T14:10:07.523Z",
	"tags": {
		"categories": [
			"opensource",
			"deep-learning",
			"theory"
		],
		"languages": []
	},
	"content": "readme.md foundations deep learning: emphasize mathematical/conceptual foundations implementations ideas(ex. torch, tensorflow) keep evolving the underlying theory must sound. anybody an interest deep learning and should try understand why things .i include neuroscience a useful conceptual foundation two reasons. , it may provide inspiration future models algorithms. second, success deep learning contribute useful hypotheses models computational neuroscience.information theory is a very useful foundation there's strong connection between data compression statistical prediction. fact, data compressors machine learning models approximate kolmogorov complexity is ultimate data compressor. might notice i haven't emphasized latest benchmark-beating paper. reason this is a theory ought be scalable means it should capable explaining why deep models generalise weshould able bootstrap explanations more complex models(ex. sequences deep models(aka rnns)). is all science is done. an excellent historical overview deep learning, would recommend reading deep learning neural networks well r. salakhutdinov's deep learning tutorials.deep learning:history:deep learning neural networks: overview (j. schmidhuber. 2015. neural networks.)optimisation:learning internal representations error propagation(d. rumelhart et al. 1996. mit press )batch normalization: accelerating deep network training reducing internal covariate shift(s. ioffe. 2015. icml.)weight normalization (salimans 2016. nips.)bayesian -propagation (w. buntine & . weigend 1991.)credit assignment through : alternatives backpropagation (y. bengio. 1993. nips.)adam: method stochastic optimization (d. kingma 2015. iclr.)understanding synthetic gradients decoupled neural interfaces(w. czarnecki 2017. corr.)regularisation:dropout: simple to prevent neural networks overfitting (n. srivastava et al. 2014. journal machine learning research.)why does unsupervised pre-training help deep learning? (d. erhan et al. 2010. journal machine learning research.)semi-supervised learning ladder networks (. rassmus et al. 2015. nips.)*tensor contraction layers parsimonious deep nets(j. kossaifi et al. 2017.)inference:uncertainty deep learning(yarin gal. 2017. university cambridge.)mixture density networks (bishop 1994)dropout a bayesian approximation(yarin gal. 2016. icml. )markov chain monte carlo variational inference: bridging gap (salimans. 2015. icml.)auto-encoding variational bayes (d. kingma & m. welling. 2014. iclr.)variational dropout the local reparameterization trick (d. kingma, t. salimans & m. welling. 2015. nips.)improved variational inference inverse autoregressive flow (d. kingma, t. salimans et al. 2017. nips.)avoiding pathologies very deep networks (d. duvenaud et al. 2014. aistats.)stochastic gradient hamiltonian monte carlo (t. chen. 2014. icml.) sparse variational methods the kullback-leibler divergence between stochastic processes(. matthews et al. 2016. aistats.)representation learning:representation learning: review new perspectives (y. bengio et al. 2013. ieee transactions pattern analysis machine intelligence.)deep learning representations unsupervised transfer learning (y. bengio. 2012. icml.)learning invariant feature hierarchies (y. lecun. 2012. eccv workshops.)deep generative models:learning deep generative models(salakhutdinov. 2015. annual review statistics its application.)learning disentangled representations semi-supervised deep generative models (n. siddarth et al. 2017.)generative adversarial nets (. goodfellow et al. 2014. nips.)* unifying deep generative models(z. hu et al. 2017.)continual learning:long short-term memory (s. hochreiter & j. schmidhuber. 1997. neural computation.)overcoming catastrophic forgetting neural networks (j. kirkpatrick et al. 2017. pnas.)hyperparameter optimization:taking human of loop: review bayesian optimization (b. shahriari et al. 2016. proceedings the ieee.)convolution evolution (c. fernando et al. 2016. gecco.)mathematics:optimisation:simple explanation the -free-lunch theorem its implications (y. ho. 2002. journal optimization theory applications.) loss surfaces multilayer networks(y lecun et al. 2015. aistats.) loss surface deep wide neural networks(q. nguyen 2017)qualitatively characterizing neural network optimization problems (. goodfellow et al. 2015. iclr.) physical systems behind optimization (l. yang et al. 2017.) differential equation modeling nesterovs accelerated gradient method(w. su 2016. journal machine learning research.)electron-proton dynamics deep learning(zhang 2017. corr.)sharp minima generalize deep nets (l. dinh et al. 2017. icml.)representation learning: mathematical theory deep convolutional neural networks feature extraction(wiatowski 2016. corr.)spectral representations convolutional neural networks(rippl 2015. nips.)learning theory:distribution-specific hardness learning neural networks(shamir 2017. corr.)lessons the rademacher complexity deep learning(sokolic 2016.iclr.) the ability neural nets express distributions (h. lee et al. 2017.)empirical risk minimization learning theory(vapnik 1991. nips.)dataset shift(storkey 2013) the ability neural nets express distributions h. lee, r. ge, t. ma, . risteski & s. arora, 2017)probably approximately correct learning (r. schapire. cos 511: foundations machine learning. 2006.)rademacher complexity (m. balcan. cs 8803 - machine learning theory. 2011.)learning behaviour:exact solutions the nonlinear dynamics learning deep linear neural networks (. saxe et al. 2013. corr.)unsupervised learning:why does deep learning ? - a perspective group theory (. paul & s. venkatasubramanian. 2015. iclr.)information theory:shannon information kolmogorov complexity (grunwald 2010)discovering neural nets low kolmogorov complexity(schmidhuber 1997. neural networks.)opening black box deep neural networks via information (schwartz-ziv 2017.)neuroscience:towards integration deep learning neuroscience(marblestone 2016. frontiers computational neuroscience.)equilibrium propagation(scellier 2016. frontiers computational neuroscience.)towards biologically plausible deep learning(bengio 2015. corr.)random synaptic feedback weights support error backpropagation deep learning(lillicrap 2016. nature communications.)towards deep learning spiking neurons(mesnard 2016. nips.)towards deep learning segregated dendrites(guergiuev 2017)variational learning recurrent spiking networks(rezende 2011. nips.) view neural networks dynamical systems(cessac 2009. . j. bifurcation chaos)convolutional network layers map function the human visual system (m. eickenberg. 2016. neuroimage elsevier.)cortical algorithms perceptual grouping (p. roelfsema. 2006. annual review neuroscience.)statistical physics:phase transitions neural networks (w. kinzel. 1997. universitat weiburg.)convolutional neural networks arise ising models restricted boltzmann machines (s. pai)non-equilibrium statistical mechanics: a paradigmatic model biological transport (t. chou et al. 2011.)replica theory spin glasses (f. morone et al. 2014.)note 1: are many love quoting richard feynman albert einstein whenever suits purpose. however, feynman's popular quote:' i cannot create, do understand' has been taken of context many ai researchers. are many things can build we 't understand many things can't build we understand very . take non-constructive proof mathematical physics example. this follows it's important create, it's essential understand. fact, think makes more sense consider perspective marie curie: \"nothing life is be feared, is to understood. is time understand more, that may fear less.\"note 2: is work progress. have more papers add."
}