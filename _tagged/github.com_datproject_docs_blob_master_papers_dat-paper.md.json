{
	"_id": "14420084",
	"site": "https://github.com/datproject/docs/blob/master/papers/dat-paper.md",
	"title": "Dat – Distributed Dataset Synchronization and Versioning",
	"author": "ColinWright",
	"date": "2017-06-13T13:56:31.670Z",
	"tags": {
		"categories": [
			"opensource"
		],
		"languages": []
	},
	"content": "title date author dat - distributed dataset synchronization versioning may 2017 maxwell ogden, karissa mckelvey, mathias buus madsen, code science abstractdat is protocol designed syncing folders data, if are large changing constantly. dat uses cryptographically secure register changes prove the requested data version is distributed. byte range any file's version be efficiently streamed a dat repository a network connection. consumers choose fully partially replicate contents a remote dat repository, can subscribe live changes. ensure writer reader privacy, dat uses public key cryptography encrypt network traffic. group dat clients connect each to form public private decentralized network exchange data between each . a reference implementation is provided javascript.1. backgroundmany datasets are shared online today using http ftp, lack built support version control content addressing data. results link rot content drift files are moved, updated deleted, leading an alarming rate disappearing data references areas such published scientific literature.cloud storage services s3 ensure availability data, they a centralized hub--spoke networking model are therefore limited their bandwidth, meaning popular files be very expensive share. services dropbox google drive provide version control synchronization top cloud storage services fixes many issues broken links rely proprietary code services requiring users store data centralized cloud infrastructure has implications cost, transfer speeds, vendor lock- and user privacy.distributed file sharing tools become faster files become more popular, removing bandwidth bottleneck making file distribution cheaper. also link resolution discovery systems can prevent broken links meaning the original source goes offline backup sources be automatically discovered. however file sharing tools today are supported web browsers, not good privacy guarantees, do provide mechanism updating files without redistributing new dataset could mean entire redownloading data already .2. datdat is dataset synchronization protocol does assume dataset is static that entire dataset be downloaded. main reference implementation is available npm npm install dat -g. protocol is agnostic the underlying transport e.g. could implement dat carrier pigeon. key properties the dat design are explained this section.2.1 content integrity - data publisher integrity is verified through of signed hashes the content.2.2 decentralized mirroring - users sharing same dat automatically discover each and exhange data a swarm.2.3 network privacy - dat provides certain privacy guarantees including end--end encryption.2.4 incremental versioning - datasets be efficiently synced, in real , to peers.2.1 content integritycontent integrity means being able verify data received is exact same version the data you expected. is imporant a distributed system this mechanism catch incorrect data sent bad peers. also has implications reproducibility it lets refer a specific version a dataset.link rot, links online stop resolving, content drift, data changes the link the data remains same, are common issues data analysis. example, day file called data.zip might change, a typical http link the file does include hash the content, provide way get updated metadata, clients only the http link no to check the file changed without downloading entire file again. referring a file the hash its content is called content addressability, lets users only verify the data receive is version the data want, also lets cite specific versions the data referring a specific hash.dat uses blake2b [@aumasson2013blake2] cryptographically secure hashes address content. hashes are arranged a merkle tree [@mykletun2003providing], tree where each non-leaf node is hash all child nodes. leaf nodes contain pieces the dataset. due the properties secure cryptographic hashes top hash only produced all data below matches exactly. two trees matching top hashes you that other nodes the tree must match well, you conclude your dataset is synchronized. trees are chosen the primary data structure dat they a number properties allow efficient access subsets the metadata, allows dat work efficiently a network connection.dat linksdat links are ed25519 [@bernstein2012high] public keys have length 32 bytes (64 characters hex encoded). can represent dat link the following ways dat clients be able understand :the standalone public key:8e1c7189b1b2dbb5c4ec2693787884771201da9...using dat:// protocol:dat://8e1c7189b1b2dbb5c4ec2693787884771... part an http url:https://datproject.org/8e1c7189b1b2dbb5... messages the dat protocol are encrypted signed using public key during transport. means unless know public key (e.g. unless dat link was shared you) you not able discover communicate any member the swarm that dat. anyone the public key verify messages (such entries a dat stream) were created a holder the private key.every dat repository has corresponding private key kept your home folder never shared. dat never exposes either public private key the network. during discovery phase blake2b hash the public key is used the discovery key. means the original key is impossible discover (unless was shared publicly through separate channel) since the hash the key is exposed publicly.dat does provide authentication mechanism this . instead provides capability system. anyone the dat link is currently considered able discover access data. not share dat links publicly you not them be accessed.hypercore hyperdrive dat storage, content integrity, networking protocols are implemented a module called hypercore. hypercore is agnostic the format the input data, operates any stream binary data. the dat case synchronizing datasets use file system module top hypercore called hyperdrive.dat has layered abstraction that users use hypercore directly have full control how model data. hyperdrive works when data be represented files a filesystem, is main case dat.hypercore registershypercore registers are core mechanism used dat. are binary append- streams whose contents are cryptographically hashed signed therefore be verified anyone access the public key the writer. are implemenation the concept known a register, digital ledger can trustdat uses registers, content metadata. content register contains files your repository metadata contains metadata the files including name, size, last modified , etc. dat replicates both synchronizing another peer. files are added dat, each file gets split into number chunks, the chunks are arranged a merkle tree, is used later version control replication processed.2.2 decentralized mirroringdat is peer peer protocol designed exchange pieces a dataset amongst swarm peers. soon a peer acquires first piece data the dataset can choose become partial mirror the dataset. someone else contacts and needs piece have, can choose share . this happen simultaneously while peer is still downloading pieces want others.source discovery important aspect mirroring is source discovery, techniques peers to find each . source discovery means finding ip port data sources online have copy that data are looking . you then connect them begin exchanging data. using source discovery techniques dat is able create network where data be discovered if original data source disappears.source discovery happen many kinds networks, long you model following actions:join(key, [port]) - begin performing regular lookups an interval key. specify port you to announce you share key well.leave(key, [port]) - stop looking key. specify port stop announcing you share key well.foundpeer(key, ip, port) - called a peer is found a lookup the dat implementation implement above actions top three types discovery networks:dns name servers - internet standard mechanism resolving keys addressesmulticast dns - useful discovering peers local networkskademlia mainline distributed hash table - less central points failure, increases probability dat working if dns servers are unreachableadditional discovery networks be implemented needed. chose above three a starting point have complementary mix strategies increase probability source discovery. additionally can specify dat via https link, runs dat protocol \"single-source\" mode, meaning above discovery networks are used, instead that https server is used the peer.peer connections the discovery phase, dat should a list potential data sources try contact. dat uses either tcp, http utp [@rossi2010ledbat]. utp uses ledbat is designed not up available bandwidth a network (e.g. that people sharing wifi still the internet), is still based udp works nat traversal techniques udp hole punching. http is support compatibility static file servers web browser clients. note these are protocols support the reference dat implementation, the dat protocol itself is transport agnostic. an http source is specified dat prefer one other sources. otherwise dat gets ip port a potential tcp utp source tries connect using both protocols. one connects , dat aborts other . if none connect, dat try again until decides source is offline unavailable then stops trying connect them. sources dat is able connect go a list known sources, that internet connection goes down dat use list reconnect known sources again quickly. dat gets lot potential sources picks handful random try connect and keeps rest around additional sources use later case decides needs more sources.once duplex binary connection a remote source is open dat layers the hypercore protocol, message based replication protocol allows peers communicate a stateless channel request exchange data. open separate replication channels many peers once allows clients parallelize data requests across entire pool peers have established connections .2.3 network privacy the web today, ssl, is guarantee the traffic between computer the server is private. long you trust server not leak logs, attackers intercept network traffic not able read http traffic exchanged between and server. is fairly straightforward model clients have trust single server some domain. is inherent tradeoff peer peer systems source discovery vs. user privacy. more sources contact ask some data, more sources trust keep you asked private. goal is have dat configurable respect this tradeoff allow application developers meet own privacy guidelines. is to client programs make design decisions around discovery networks trust. example a dat client decides use bittorrent dht discover peers, they are searching a publicly shared dat key (e.g. key cited publicly a published scientific paper) known contents, because the privacy design the bittorrent dht becomes public knowledge key client is searching .a client choose only discovery networks certain privacy guarantees. example client only connect an approved list sources they trust, similar ssl. long they trust each source, encryption built the dat network protocol prevent dat key are looking from being leaked.2.4 incremental versioninggiven stream binary data, dat splits stream chunks, hashes each chunk, arranges hashes a specific type merkle tree allows certain replication properties.dat is able fully partially synchronize streams a distributed setting if stream is being appended . this is accomplished using messaging protocol traverse merkle tree remote sources fetch strategic set nodes. due the low level message oriented design the replication protocol different node traversal strategies be implemented. are types versioning performed automatically dat. metadata is stored a folder called .dat the root folder a repository, data is stored normal files the root folder.metadata versioningdat tries much possible act a -to- mirror the state a folder all 's contents. importing files, dat uses sorted depth- recursion list the files the tree. each file finds, grabs filesystem metadata (filename, stat object, etc) checks there is already entry this filename this exact metadata already represented the dat repository metadata. the file this metadata matches exactly newest version the file metadata stored dat, this file be skipped ( change). the metadata differs the current existing (or are entries this filename all the history), this metadata entry be appended the 'latest' version this file the append- sleep metadata content register (described below).content versioning addition storing historical record filesystem metadata, content the files themselves are capable being stored a version controlled manner. default storage system used dat stores files files. has advantage being very straightforward users understand, the downside not storing old versions content default. contrast other version control systems git, dat default stores current set checked files disk the repository folder, old versions. does store previous metadata old versions .dat. git example stores previous content versions all previous metadata versions the .git folder. dat is designed larger datasets, it stored previous file versions .dat, the .dat folder easily fill the users hard drive inadverntently. therefore dat has multiple storage modes based usage.hypercore registers include optional data file stores chunks data. dat, the metadata.data file is used, the content.data file is used. default behavior is store current files as normal files. you to run 'archival' node keeps previous versions, can configure dat use content.data file instead. example, a shared server lots storage probably to store versions. however a workstation machine is accessing subset one version, default mode storing metadata plus current set downloaded files is acceptable, you the server has full history.merkle treesregisters dat a specific method encoding merkle tree where hashes are positioned a scheme called binary -order interval numbering just \"bin\" numbering. is a specific, deterministic of laying the nodes a tree. example tree 7 nodes always arranged this:0 12 34 56 dat, hashes the chunks files are always numbers, the wide end the tree. the above tree had four original values become even numbers:chunk0 -> 0chunk1 -> 2chunk2 -> 4chunk3 -> 6 the resulting merkle tree, even odd nodes store different information:evens - list data hashes [chunk0, chunk1, chunk2, ...]odds - list merkle hashes (hashes child nodes) [hash0, hash1, hash2, ...] two lists interleaved a single register such the indexes (position) the register are same the bin numbers the merkle tree. odd hashes are derived hashing two child nodes, e.g. given hash0 is hash(chunk0) hash2 is hash(chunk1), hash1 is hash(hash0 + hash2). example register two data entries look something this (pseudocode):0. hash(value0)1. hash(hash(chunk0) + hash(chunk1))2. hash(value1) is possible the -order merkle tree have multiple roots once. root is defined a parent node a full set child node slots filled below .for example, tree hash 2 roots (1 4)0 124 tree hash root (3):0 12 34 56 one has root (1):0 12replication example section describes high level replication flow a dat. note the low level details are available reading sleep section below. the sake illustrating this works practice a networked replication scenario, consider folder two files:bat.jpgcat.jpg send files another machine using dat, would add to dat repository splitting into chunks constructing sleep files representing chunks filesystem metadata.let's assume cat.jpg produces three chunks, bat.jpg produces four chunks, each around 64kb. dat stores a representation called sleep, here will show pseudo-representation the purposes illustrating replication process. seven chunks sorted a list this:bat-1bat-2bat-3cat-1 cat-2cat-3 chunks each hashed, the hashes arranged a merkle tree ( content register):0 - hash(bat-1) 1 - hash(0 + 2)2 - hash(bat-2) 3 - hash(1 + 5)4 - hash(bat-3) 5 - hash(4 + 6)6 - hash(cat-1)8 - hash(cat-2) 9 - hash()10 - hash(cat-3)next calculate root hashes our tree, this case 3 9. then hash together, cryptographically sign hash. signed hash can used verify nodes the tree, the signature proves was produced us, holder the private key this dat. tree is the hashes the contents the photos. is a second merkle tree dat generates represents list files their metadata looks something this ( metadata register):0 - hash({contentregister: '9e29d624...'}) 1 - hash(0 + 2)2 - hash({\"bat.png\", : 0, length: 3})4 - hash({\"cat.png\", : 3, length: 3}) first entry this feed is special metadata entry tells dat address the second feed ( content register). note node 3 is included yet, 3 is hash 1 + 5, 5 does exist yet, will written a later update. we're ready send metadata the peer. first message is register message the key was shared this dat. let's call ourselves alice the peer bob. alice sends bob want message declares want nodes the file list ( metadata register). bob replies a single message indicates has 2 nodes data. alice sends three request messages, for each leaf node (0, 2, 4). bob sends three data messages. first data message contains content register key, hash the sibling, this case node 2, hash the uncle root 4, well a signature the root hashes ( this case 1, 4). alice verifies integrity this data message hashing metadata received the content register metadata produce hash node 0. then hash hash 0 the hash 2 was included reproduce hash 1, hashes 1 the value 4 received they use signature received verify was same data. the next data message is received, similar process is performed verify content. alice has full list files the dat, decides only to download cat.png. alice knows want blocks 3 through 6 the content register. alice sends another register message the content key open new replication channel the connection. alice sends three request messages, for each blocks 4, 5, 6. bob sends three data messages the data each block, well the hashes needed verify content a similar the process described above the metadata feed.3. sleep specification section is technical description the sleep format intended implementers. sleep is the -disk format dat produces uses. is set 9 files hold of metadata needed list contents a dat repository verify integrity the data receive. sleep is designed work rest, allowing servers be plain http file servers serving static sleep files, meaning can implement dat protocol client using http a static http file server the backend.sleep files contain metadata the data inside dat repository, including cryptographic hashes, cryptographic signatures, filenames file permissions. sleep format is specifically engineered allow efficient access subsets the metadat /or data the repository, on very large repositories, enables dat's peer peer networking be fast. acronym sleep is slumber related pun rest stands syncable lightweight event emitting persistence. event emitting part refers how sleep files are append- in nature, meaning grow time new updates be subscribed as realtime feed events through dat protocol. sleep version described here, used dat of 2017 is sleep v2. sleep v1 is documented http://specs.okfnlabs.org/sleep.sleep filessleep is set 9 files should stored the following names. dat, files are stored a folder called .dat the top level the repository.metadata.keymetadata.signaturesmetadata.bitfieldmetadata.treemetadata.datacontent.keycontent.signaturescontent.bitfieldcontent.tree files prefixed content store metadata the primary data a dat repository, example raw binary contents the files. files prefixed metadata store metadata the files the repository, example filenames, file sizes, file permissions. content metadata files are both hypercore registers, making sleep set two hypercore registers.sleep file headers following structured binary format is used signatures, bitfield, tree files. header contains metadata well information needed decode rest the files the header. sleep files are designed be easy append data at end, easy read arbitrary byte offsets the middle, are relatively flat, simple files rely the filesystem the heavy lifting.sleep files are laid like :<32 byte header><fixed-size entry 1><fixed-size entry 2><fixed-size entry ...><fixed-size entry n>32 byte header4 bytes - magic byte (value varies depending which file, used quickly identify file type is)1 byte - version number the file header protocol, current version is 02 byte uint16be - entry size, describes long each entry the file is1 byte - length prefix bodyrest 32 byte header - string describing key algorithm ( dat 'ed25519'). length this string matches length the previous length prefix field. string must fit within 32 byte header limitation (24 bytes reserved string). unused bytes should filled zeroes.possible values the dat implementation the body field are:ed25519blake2b calculate offset some entry position, read header get entry size, do 32 + entrysize * entryindex. calculate many entries are a file, can the entry size the filesize disk do (filesize - 32) / entrysize. mentioned above, signatures, bitfield tree are three sleep files. are additional files, key, data, do contain sleep file headers store plain serialized data easy access. key stores public key is described the signatures file, data stores raw chunk data the tree file contains hashes metadata .file descriptionskey public key used verify signatures the signatures file. stored binary a single buffer written disk. find what format key is stored this file, read header signatures. dat, 's always ed25519 public key, other implementations specify key types using string value that header.tree sleep formatted 32 byte header data entries representing serialized merkle tree based the data the data storage layer. the fixed size nodes written in-order tree notation. header algorithm string tree files is blake2b. entry size is 40 bytes. entries are formatted this:<32 byte header> <4 byte magic string: 0x05025702> <1 byte version number: 0> <2 byte entry size: 40> <1 byte algorithm name length prefix: 7> <7 byte algorithm name: blake2b> <17 zeroes><40 byte entries> <32 byte blake2b hash> <8 byte uint64be children leaf byte length> children leaf byte length is byte size containing sum byte length all leaf nodes the tree below node. file uses in-order notation, meaning entries are leaf nodes odd entries are parent nodes (non-leaf). prevent pre-image attacks, hashes start a byte type descriptor:0 - leaf1 - parent2 - root calculate leaf node entries ( hashes the data entries) hash data:blake2b( <1 byte type> 0 <8 bytes uint64be> length entry data <entry data>) we this 32 byte hash write to tree 40 bytes this:<32 bytes> blake2b hash<8 bytes uint64be> length datanote the uint64 length data is included both the hashed data written the end the entry. is expose more metadata dat advanced cases such verifying data length sparse replication scenarios. calculate parent node entries ( hashes the leaf nodes) hash data:blake2b( <1 byte> 1 <8 bytes uint64be> left child length + right child length <32 bytes> left child hash <32 bytes> right child hash) we this 32 byte hash write to tree 40 bytes this:<32 bytes> blake2b hash<8 bytes uint64be> left child length + right child length reason tree entries contain data lengths is allow sparse mode replication. encoding lengths ( including lengths all hashes) means can verify merkle subtrees independent the rest the tree, happens during sparse replication scenarios. tree file corresponds directly the data file.data data file is included the sleep format the metadata.* prefixed files contains filesystem metadata not actual file data. the content.* files, data is stored externally ( dat is stored normal files the filesystem not a sleep file). however can configure dat use content.data file you and will still . if want store full history all versions all files, using content.data file provide guarantee, would the disadvantage storing files chunks merged one huge file ( as user friendly). data file does contain sleep file header. just contains bunch concatenated data entries. entries are written the same order they appear the tree file. read data file, decode tree file for every leaf the tree file can calculate data offset the data described that leaf node the data file.index lookup example, we wanted seek a specific entry offset ( entry 42):, read header the tree file get entry size, do 32 + entrysize * 42 get raw tree index: 32 + (40 * 42)since want leaf entry ( node the -order layout), multiply entry index 2:32 + (40 * (42 * 2))read 40 bytes that offset the tree file get leaf node entry.read last 8 bytes the entry get length the data entry calculate offset where the data file entry begins, need sum the lengths all earlier entries the tree. most efficient to this is sum the previous parent node (non-leaf) entry lengths. can sum leaf node lengths, parent nodes contain sum their childrens lengths it's more efficient use parents. during dat replication, nodes are fetched part the merkle tree verification you already them locally. is log(n) operation where n is entry index. entries are small therefore easily cacheable.once get offset, use length decoded above read n bytes (where n is decoded length) the offset the data file. can verify data integrity using 32 byte hash the tree entry.byte lookup above method illustrates to resolve chunk position index a byte offset. can do reverse operation, resolving byte offset a chunk position index. is used stream arbitrary random access regions files sparse replication scenarios., you start calculating current merkle rootseach node the tree (including root nodes) stores aggregate file size all byte sizes the nodes below . so roots cumulatively describe possible byte ranges this repository.find root contains byte range the offset are looking and the node information all that nodes children using index lookup method, recursively repeat step until find lowest down child node describes byte range. chunk described this child node contain byte range are looking . you use byteoffset property the stat metadata object seek the right position the content the start this chunk.metadata overheadusing scheme, you write 4gb data using average 64kb data chunks (note: chunks be variable length do need be same size), tree file be around 5mb (0.0125% overhead).signatures sleep formatted 32 byte header data entries being 64 byte signatures.<32 byte header> <4 byte magic string: 0x05025701> <1 byte version number: 0> <2 byte entry size: 64> <1 byte algorithm name length prefix: 7> <7 byte algorithm name: ed25519> <17 zeroes><64 byte entries> <64 byte ed25519 signature>every the tree is updated sign current roots the merkle tree, append to signatures file. signatures file starts no entries. each a leaf is appended the tree file (aka whenever data is added a dat), take root hashes the current state the merkle tree hash sign , then append as new entry the signatures file.ed25519 sign( blake2b( <1 byte> 2 // root type (every root node left--right) { <32 byte root hash> <8 byte uint64be root tree index> <8 byte uint64be child byte lengths> } )) reason hash the root nodes is the blake2b hash above is calculateable you all the pieces data required generate the intermediate hashes. is crux dat's data integrity guarantees.bitfield sleep formatted 32 byte header followed a series 3328 byte long entries.<32 byte header> <4 byte magic string: 0x05025700> <1 byte version number: 0> <2 byte entry size: 3328> <1 byte algorithm name length: 0> <1 byte algorithm name: 0> <24 zeroes><3328 byte entries> // (2048 + 1024 + 256) bitfield describes pieces data have, which nodes the tree file been written. file exists an index the tree data quickly figure which pieces data have are missing. file be regenerated you delete , so is considered materialized index. bitfield file actually contains three bitfields different sizes. bitfield (aka bitmap) is defined a set bits where each bit (0 1) represents you or not a piece data that bit index. if is dataset 10 cat pictures, you pictures 1, 3, 5 are missing rest, bitfield look 1010100000.each entry contains three objects:data bitfield (1024 bytes) - 1 bit for each data entry you synced (1 every entry data).tree bitfield (2048 bytes) - 1 bit every tree entry ( nodes tree)bitfield index (256 bytes) - is index the data bitfield makes efficient figure which pieces data are missing the data bitfield without having do linear scan. data bitfield is 1kb somewhat arbitrarily, the idea is because filesystems in 4kb chunk sizes, can fit data, tree index less 4kb data efficient writes the filesystem. tree index sizes are based the data size ( tree has twice entries the data, odd even nodes vs even nodes tree, index is always 1/4th size). generate index, pairs 2 bytes a from data bitfield, check all bites the 2 bytes are same, generate 4 bits index metadata  every 2 bytes data (hence 1024 bytes data ends as 256 bytes index). you generate 2 bit tuple the 2 bytes data: (data is 1's) [1,1] (data is 0's) [0,0] (data is all same) [1, 0] index itself is in-order binary tree, a traditional bitfield. generate tree, take tuples generate above then write into tree the following example, where non-leaf nodes are generated using above scheme looking the results the relative child tuples each odd parent tuple:// e.g. 16 bytes (8 tuples) // sparsely replicated data0 - [00 00 00 00]1 - [10 10 10 10]2 - [11 11 11 11] tuples entry 1 above are [1,0] the relative child tuples are uniform. the following example, non-leaf nodes are [1,1] their relative children are uniform ([1,1])// e.g. 32 bytes (16 tuples) // fully replicated data ( 1's)0 - [11 11 11 11]1 - [11 11 11 11]2 - [11 11 11 11]3 - [11 11 11 11]4 - [11 11 11 11]5 - [11 11 11 11]6 - [11 11 11 11]using scheme, represent 32 bytes data takes most 8 bytes index. this example compresses nicely its contiguous ones disk, similarly an empty bitfield would all zeroes. you write 4gb data using average 64kb data chunk size, bitfield be most 32kb.metadata.data file is used store content described the rest the metadata.* hypercore sleep files. whereas content.* sleep files describe data stored the actual data cloned the dat repository filesystem, metadata data feed is stored inside .dat folder along the rest the sleep files. contents this file is series versions the dat filesystem tree. this is hypercore data feed, 's an append log binary data entries. challenge is representing tree an dimensional to it representable a hypercore register. example, imagine three files:~/dataset $ lsfigures graph1.png graph2.pngresults.csv1 directory, 3 files want take structure map to serialized representation gets written an append log a that still allows efficient random access file path. do , we convert filesystem metadata entries a feed this:{ \"path\": \"/results.csv\", children: [[]], sequence: 0}{ \"path\": \"/figures/graph1.png\", children: [[0], []], sequence: 1}{ \"path\": \"/figures/graph2\", children: [[0], [1]], sequence: 2}filename resolutioneach sequence represents adding of files the register, at sequence 0 filesystem state has single file, results.csv it. sequence 1, are 2 files added the register, at sequence 3 files are finally added. children field represents shorthand of declaring other files every level the directory hierarchy exist alongside file being added that revision. example the of sequence 1, children is [[0], []]. first sub-array, [0], represents first folder the path, is root folder /. this case [0] means root folder this point time has single file, file is subject sequence 0. second subarray is empty [] there are other existing files the second folder the path, figures. look a file filename, fetch latest entry the log, use children metadata that entry look the longest common ancestor based the parent folders the filename are querying. can recursively repeat operation until find path entry are looking (or exhaust options means file does exist). is o(number slashes your path) operation. example, you wanted look /results.csv given above register, would start grabbing metadata sequence 2. longest common ancestor between /results.csv /figures/graph2 is /. then grab corresponding entry the children array /, which this case is first entry, [0]. then repeat with of chilren entries until find child is closer the entry are looking . in example, first entry happens be match are looking .you also perform lookups relative a point time starting a specific sequence number the register. example get state some file relative an old sequence number, similar checking an old version a repository git.data serialization format the metadata.data file is follows:<header><node 1><node 2><node ...><node n>each entry the file is encoded using protocol buffers [@varda2008protocol]. first message write the file is a type called header uses schema:message header { required string type = 1; optional bytes content = 2;} is used declare pieces metadata used dat. includes type string the value hyperdrive content binary value holds public key the content register this metadata register represents. you share dat, metadata key is main key gets used, the content register key is linked here the metadata. the header file contain many filesystem node entries:message node { required string path = 1; optional stat value = 2; optional bytes children = 3;} node object has three fieldspath - string the absolute file path this file.stat - stat encoded object representing file metadatachildren - compressed list the sequence numbers described earlier children value is encoded starting the nested array sequence numbers, e.g. [[3], [2, 1]]. then sort individual arrays, this case resulting [[3], [1, 2]]. then delta compress each subarray storing difference between each integer. this case would [[3], [1, 1]] 3 is 3 more 0, 1 is 1 more than 0, 2 is 1 more 1. we write delta compressed subarrays write using variable width integers (varints), using repeating pattern this, for each array:<varint first subarray element length><varint the delta this array><varint the next delta ...><varint the last delta> encoding is designed efficiency it reduces filesystem path metadata down a series small integers. stat objects this encoding:message stat { required uint32 mode = 1; optional uint32 uid = 2; optional uint32 gid = 3; optional uint64 size = 4; optional uint64 blocks = 5; optional uint64 offset = 6; optional uint64 byteoffset = 7; optional uint64 mtime = 8; optional uint64 ctime = 9;} are field defintions:mode - posix file mode bitmaskuid - posix user idgid - posix group idsize - file size bytesblocks - number data chunks make this fileoffset - data feed entry index the chunk this filebyteoffset - data feed file byte offset the chunk this filemtime - posix modified_at mtime - posix created_at 4. dat network protocol sleep format is designed allow sparse replication, meaning can efficiently download the metadata data required resolve single byte region a single file, makes dat suitable a wide variety streaming, real and large dataset cases. take advantage this, dat includes network protocol. is message based stateless, making possible implement a variety network transport protocols including udp tcp. both metadata content registers sleep share exact same replication protocol.individual messages are encoded using protocol buffers there are ten message types using following schema:wire protocol the wire messages are packed the following lightweight container format<varint - length rest message> <varint - header> <message> header value is single varint has pieces information, integer type declares 4-bit message type (used below), a channel identifier, 0 metadata 1 content. generate varint, bitshift 4-bit type integer onto end the channel identifier, e.g. channel << 4 | <4-bit-type>.registertype 0, should the message sent a channel.discoverykey - blake2b keyed hash the string 'hypercore' using public key the metadata register the key.nonce - 32 bytes random binary data, used our encryption schememessage register { required bytes discoverykey = 1; optional bytes nonce = 2;}handshaketype 1. overall connection handshake. should sent after register message the channel (metadata).id - 32 byte random data used a identifier this peer the network, useful checking you are connected yourself another peer more oncelive - whether not want operate live (continuous) replication mode end the initial syncmessage handshake { optional bytes id = 1; optional bool live = 2;}statustype 2. message indicating state changes. used indicate whether are uploading /or downloading.initial state uploading/downloading is true. both ends are downloading not live is safe consider stream ended.message status { optional bool uploading = 1; optional bool downloading = 2;}type 3. you tell other peer chunks data have don't . you should send messages peers have expressed interest this region want messages.start - you specify start, means are telling other side only 1 chunk the position the value start.length - you specify length, can describe range values you all , starting start.bitfield - you like send range sparse data haves/don't haves via bitfield, relative start.message { required uint64 start = 1; optional uint64 length = 2 [default = 1]; optional bytes bitfield = 3;} sending bitfields must run length encode . the encoded bitfield is series compressed uncompressed bit sequences. sequences start a header is varint. the last bit is set the varint ( is odd number) a header represents compressed bit sequence.compressed-sequence = varint( byte-length--sequence << 2 | bit << 1 | 1) the last bit is set a header represents non compressed sequenceuncompressed-sequence = varint( byte-length--bitfield << 1 | 0) + (bitfield)unhavetype 4. you communicate you deleted removed chunk used have.message unhave { required uint64 start = 1; optional uint64 length = 2 [default = 1];}type 5. you ask other peer subscribe to messages a region chunks. length value defaults infinity feed.length ( not live).message { required uint64 start = 1; optional uint64 length = 2;}unwanttype 6. you ask unsubscribe have messages a region chunks the peer. should unwant previously wanted regions, if do unwant something hasn't been wanted won't any effect. length value defaults infinity feed.length ( not live).message unwant { required uint64 start = 1; optional uint64 length = 2;}requesttype 7. request single chunk data.index - chunk index the chunk want. should ask indexes you received have messages .bytes - can optimistically specify byte offset, in case remote is able resolve chunk this byte offset depending their merkle tree state, will ignore index send chunk resolves this byte offset instead. if cannot resolve byte request, index be used.hash - you want hash the chunk not chunk data itself.nodes - 64 bit long bitfield representing parent nodes have. nodes bitfield is optional optimization reduce amount duplicate nodes exchanged during replication lifecycle. indicates parents have don't . you a maximum 64 parents can specify. uint64 protocol buffers is implemented a varint, the wire does take 64 bits most cases. first bit is reserved signify whether not need signature response. rest the bits represent whether not have (1) don't (0) information this node already. ordering is determined walking parent, sibling the tree the to root.message request { required uint64 index = 1; optional uint64 bytes = 2; optional bool hash = 3; optional uint64 nodes = 4;}canceltype 8. cancel previous request message you haven't received yet.message cancel { required uint64 index = 1; optional uint64 bytes = 2; optional bool hash = 3;}datatype 9. sends single chunk data the peer. can send in response a request unsolicited it's own a friendly gift. data includes of merkle tree parent nodes needed verify hash chain the up the merkle roots this chunk. you produce direct parents hashing chunk, the roots 'uncle' hashes are included ( siblings all the parent nodes).index - chunk position this chunk.value - chunk binary data. empty you are sending the hash.node.index - index this chunk in-order notationnode.hash - hash this chunknode.size- aggregate chunk size all children below node ( sum all chunk sizes all children)signature - you are sending root node, root nodes must the signature included.message data { required uint64 index = 1; optional bytes value = 2; repeated node nodes = 3; optional bytes signature = 4; message node { required uint64 index = 1; required bytes hash = 2; required uint64 size = 3; }}5. existing dat is inspired a number features existing systems.gitgit popularized idea a directed acyclic graph (dag) combined a merkle tree, way represent changes data where each change is addressed the secure hash the change plus ancestor hashes a graph. provides way trust data integrity, the way specific hash be derived another peer is they the same data change history required reproduce hash. is important reproducibility it lets trust a specific git commit hash refers a specific source code state.decentralized version control tools source code git provide protocol efficiently downloading changes a set files, are optimized text files have issues large files. solutions git-lfs solve by using http download large files, rather the git protocol. github offers git-lfs hosting charges repository owners bandwidth popular files. building distributed distribution layer files a git repository is difficult due design git packfiles are delta compressed repository states do easily support random access byte ranges previous file versions.bittorrentbittorrent implements swarm based file sharing protocol static datasets. data is split fixed sized chunks, hashed, then hash is used discover peers have same data. advantage using bittorrent dataset transfers is download bandwidth be fully saturated. since file is split pieces, peers efficiently discover pieces each the peers are connected have, means peer download non-overlapping regions the dataset many peers the same in parallel, maximizing network throughput.fixed sized chunking has drawbacks data changes ( lbfs above). bittorrent assumes metadata be transferred front makes impractical streaming updating content. bittorrent clients divide data 1024 pieces meaning large datasets have very large chunk size impacts random access performance (e.g. streaming video).another drawback bittorrent is due the clients advertise discover peers absence any protocol level privacy trust. a user privacy standpoint, bittorrent leaks users are accessing attempting access, does provide same browsing privacy functions systems ssl.kademlia distributed hash tablekademlia [@maymounkov2002kademlia] is distributed hash table, distributed key/value store can serve similar purpose dns servers has hard coded server addresses. clients kademlia are servers. long you at least address another peer the network, can ask for key are trying find they either it give some people talk that are more likely have .if don't an initial peer talk you, clients a bootstrap server randomly gives a peer the network start . if bootstrap server goes down, network still functions long other methods be used bootstrap peers (such sending peer addresses through side channels how .torrent files include tracker addresses try case kademlia finds peers).kademlia is distinct previous dht designs due its simplicity. uses very simple xor operation between keys its \"distance\" metric decide peers are closer the data being searched . on paper seems it wouldn't as doesn't into account things ping speed bandwidth. instead design is very simple purpose minimize amount control/gossip messages to minimize amount complexity required implement . in practice kademlia has been extremely successful is widely deployed the \"mainline dht\" bittorrent, support all popular bittorrent clients today.due the simplicity the original kademlia design number attacks such ddos /or sybil been demonstrated. are protocol extensions (beps) in certain cases mitigate effects these attacks, such bep 44 includes ddos mitigation technique. nonetheless anyone using kademlia should aware the limitations.peer peer streaming peer protocol (ppspp)ppspp (ietf rfc 7574, [@bakker2015peer]) is protocol live streaming content a peer peer network. it define specific type merkle tree allows subsets the hashes be requested a peer order reduce time-till-playback end users. bittorrent example transfers hashes front, is suitable live streaming. merkle trees are ordered using scheme call \"bin numbering\", is method deterministically arranging append- log leaf nodes an -order layout tree where non-leaf nodes are derived hashes. you to verify specific node, only need request sibling's hash all uncle hashes. ppspp is very concerned reducing round trip and -till-playback allowing many kinds optimizations, such to pack many hashes datagrams possible exchanging tree information peers.although ppspp was designed streaming video mind, ability request subset metadata a large /or streaming dataset is very desirable many types datasets.webtorrent webrtc browsers now peer peer connections directly other browsers. bittorrent uses udp sockets aren't available browser javascript, can't used -is the web.webtorrent implements bittorrent protocol javascript using webrtc the transport. includes bittorrent block exchange protocol well the tracker protocol implemented a that enable hybrid nodes, talking simultaneously both bittorrent webtorrent swarms ( a client is capable making both udp sockets well webrtc sockets, such node.js). trackers are exposed web clients http websockets.interplanetary file systemipfs is family application network protocols have peer peer file sharing data permanence baked . ipfs abstracts network protocols naming systems provide alternative application delivery platform todays web. example, instead using http dns directly, ipfs would libp2p streams ipns order gain access the features the ipfs platform.certificate transparency/secure registers uk government digital service developed concept a register they define a digital public ledger can trust. the uk government registers are beginning be piloted a to expose essential open data sets a where consumers verify data has been tampered , and allows data publishers update data sets time. design registers was inspired the infrastructure backing certificate transparency [@laurie2013certificate] project, initated google, provides service top ssl certificates enables service providers write certificates a distributed public ledger. anyone client service provider verify a certificate received is the ledger, protects against called \"rogue certificates\".6. reference implementation connection logic is implemented a module called discovery-swarm. builds discovery-channel adds connection establishment, management statistics. provides statistics such how many sources are currently connected, many and bad behaving sources been talked , and automatically handles connecting reconnecting sources. utp support is implemented the module utp-native. implementation source discovery is called discovery-channel. also run custom dns server dat clients (in addition specifying own they need ), as as dht bootstrap server. discovery servers are only centralized infrastructure need dat work the internet, they are redundant, interchangeable, never the actual data being shared, anyone run own dat still even they are unavailable. this happens discovery just manual (e.g. manually sharing ip/ports).acknowledgements work was made possible through grants the john s. james l. knight alfred p. sloan foundations.references"
}