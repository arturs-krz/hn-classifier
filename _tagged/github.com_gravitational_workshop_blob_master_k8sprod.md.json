{
	"_id": "14369181",
	"site": "https://github.com/gravitational/workshop/blob/master/k8sprod.md",
	"title": "Kubernetes Production Patterns and Anti-Patterns",
	"author": "twakefield",
	"date": "2017-06-13T13:57:07.161Z",
	"tags": {
		"categories": [
			"opensource"
		],
		"languages": []
	},
	"content": "kubernetes production patterns... anti-patterns. are going explore helpful techniques improve resiliency high availability kubernetes deployments will a at common mistakes avoid working docker kubernetes.installation, follow installation instructionsanti-pattern: mixing build environment runtime environmentlet's a at dockerfile ubuntu:14.04run apt- updaterun apt- install gccrun gcc hello.c -o /hello compiles runs simple helloworld program:$ cd prod/build$ docker build -t prod .$ docker run prodhello world are couple problems the resulting dockerfile:size$ docker images | grep prodprod latest b2c197180350 14 minutes ago 293.7 mb's almost 300 megabytes host several kilobytes the c program! are bringing package manager,c compiler lots other unnecessary tools are required run program. leads to second problem:security distribute whole build toolchain. addition that, ship source code the image:$ docker run --entrypoint=cat prod /build/hello.c#include<stdio.h>int main(){ printf(\"hello world\"); return 0;}splitting build environment run environment are going use \"buildbox\" pattern build image build environment, we use much smaller runtime environment run program$ cd prod/build-fix$ docker build -f build.dockerfile -t buildbox .note: have used -f flag specify dockerfile are going use. we a buildbox image contains build environment. can it compile c program :$ docker run -v $(pwd):/build buildbox gcc /build/hello.c -o /build/hello have used docker build time, mounted source code run compiler directly.note: docker soon support pattern natively introducing build stages the build process.update: multi-stage builds is available ce. can use much simpler ( smaller) dockerfile run image: quay.io/gravitational/debian-tall:0.0.1add hello /helloentrypoint [\"/hello\"]$ docker build -f run.dockerfile -t prod:v2 .$ docker run prod:v2hello world$ docker images | grep prodprod v2 ef93cea87a7c 17 seconds ago 11.05 mbprod latest b2c197180350 45 minutes ago 293.7 mbnote: please aware you should either plan providing needed \"shared libraries\" the runtime image \"statically build\" binaries have include needed libraries.anti pattern: zombies orphansnotice: example demonstration only on linuxorphans is quite easy leave orphaned processes running the background. let's an image have built the previous example:docker run busybox sleep 10000, let's open separate terminal locate processps uax | grep sleepsasha 14171 0.0 0.0 139736 17744 pts/18 sl+ 13:25 0:00 docker run busybox sleep 10000root 14221 0.1 0.0 1188 4 ? ss 13:25 0:00 sleep 10000 you there are fact processes: docker run sleep 1000 running a container.let's send kill signal the docker run ( as ci/cd job do long running processes):kill 14171docker run process has exited, sleep process is running!ps uax | grep sleeproot 14221 0.0 0.0 1188 4 ? ss 13:25 0:00 sleep 10000yelp engineers a answer why happens here: linux kernel applies special signal handling processes run pid 1. processes are sent signal a normal linux system, kernel first check any custom handlers process has registered that signal, otherwise fall to default behavior ( example, killing process sigterm).however, the process receiving signal is pid 1, gets special treatment the kernel; it hasn't registered handler the signal, kernel won't fall to default behavior, nothing happens. other words, your process doesn't explicitly handle signals, sending sigterm have effect all. solve (and ) issues, need simple init system has proper signal handlers specified. luckily yelp engineers built simple lightweight init system, dumb-initdocker run quay.io/gravitational/debian-tall /usr/bin/dumb-init /bin/sh -c \"sleep 10000\" you simply stop docker run process using sigterm it handle shutdown properlyanti-pattern: direct of podskubernetes pod is building block itself is durable. not pods directly production. won't rescheduled, retain data guarantee durability.instead, can deployment replication factor 1, will guarantee pods get rescheduled will survive eviction node loss.anti-pattern: using background processes$ cd prod/background$ docker build -t $(minikube ip):5000/background:0.0.1 .$ docker push $(minikube ip):5000/background:0.0.1$ kubectl create -f crash.yaml$ kubectl podsname ready status restarts agecrash 1/1 running 0 5s container appears be running, let's check our server is running :$ kubectl exec -ti crash /bin/bashroot@crash:/# root@crash:/# root@crash:/# ps uaxuser pid %cpu %mem vsz rss tty stat start commandroot 1 0.0 0.0 21748 1596 ? ss 00:17 0:00 /bin/bash /start.shroot 6 0.0 0.0 5916 612 ? s 00:17 0:00 sleep 100000root 7 0.0 0.0 21924 2044 ? ss 00:18 0:00 /bin/bashroot 11 0.0 0.0 19180 1296 ? r+ 00:18 0:00 ps uaxroot@crash:/# using probes made mistake the http server is running but is indication this the parentprocess is still running. first obvious fix is use proper init system monitor status the web service.however, let's this an opportunity use liveness probes:apiversion: v1kind: podmetadata: name: fix namespace: defaultspec: containers: - command: ['/start.sh'] image: localhost:5000/background:0.0.1 name: server imagepullpolicy: always livenessprobe: httpget: path: / port: 5000 timeoutseconds: 1$ kubectl create -f fix.yaml liveness probe fail the container get restarted.$ kubectl podsname ready status restarts agecrash 1/1 running 0 11mfix 1/1 running 1 1mproduction pattern: loggingset your logs go stdout:$ kubectl create -f logs/logs.yaml$ kubectl logs logshello, world!kubernetes docker a system plugins make sure logs sent stdout stderr getcollected, forwarded rotated.note: is of patterns the twelve factor app kubernetes supports out the box!production pattern: immutable containersevery you write something container's filesystem, activates copy write strategy. new storage layer is created using storage driver (devicemapper, overlayfs others). case active usage, can put lot load storage drivers, especially case devicemapper btrfs. sure containers write data to volumes. can tmpfs small ( tmpfs stores everything memory) temporary files:apiversion: v1kind: podmetadata: name: test-pdspec: containers: - image: busybox name: test-container volumemounts: - mountpath: /tmp name: tempdir volumes: - name: tempdir emptydir: {}anti-pattern: using latest tag not latest tag production. creates ambiguity, it's clear real version the app is. is ok use latest development purposes, although sure set imagepullpolicy always, make surekubernetes always pulls latest version creating pod:apiversion: v1kind: podmetadata: name: always namespace: defaultspec: containers: - command: ['/bin/sh', '-c', \"echo hello, world!\"] image: busybox:latest name: server imagepullpolicy: alwaysproduction pattern: pod readinessimagine situation your container takes time start. simulate , we are going write simple script:#!/bin/bashecho \"starting \"sleep 30echo \"started successfully\"python -m http.serve 5000push image start service deployment:$ cd prod/delay$ docker build -t $(minikube ip):5000/delay:0.0.1 .$ docker push $(minikube ip):5000/delay:0.0.1$ kubectl create -f service.yaml$ kubectl create -f deployment.yamlenter curl container inside cluster make sure all works:kubectl run - -t --rm cli --image=tutum/curl --restart=nevercurl http://delay:5000<!doctype html>... will notice there's connection refused error, you try access for first 30 seconds.update deployment simulate deploy:$ docker build -t $(minikube ip):5000/delay:0.0.2 .$ docker push $(minikube ip):5000/delay:0.0.2$ kubectl replace -f deployment-update.yaml the next window, let's try to if got service downtime:curl http://delay:5000curl: (7) failed connect delay port 5000: connection refused've got production outage despite setting maxunavailable: 0 our rolling update strategy! happened kubernetes did know startup delay readiness the service.let's fix by using readiness probe:readinessprobe: httpget: path: / port: 5000 timeoutseconds: 1 periodseconds: 5readiness probe indicates readiness the pod containers kubernetes take into account doing deployment:$ kubectl replace -f deployment-fix.yaml time will no downtime.anti-pattern: unbound quickly failing jobskubernetes provides useful tool schedule containers perform -time task: jobshowever, is problem:apiversion: batch/v1kind: jobmetadata: name: badspec: template: metadata: name: bad spec: restartpolicy: never containers: - name: box image: busybox command: [\"/bin/sh\", \"-c\", \"exit 1\"]$ cd prod/jobs$ kubectl create -f job.yaml are going observe race create hundreds containers the job retrying forever:$ kubectl describe jobs name:badnamespace:defaultimage(s):busyboxselector:controller-uid=18a6678e-11d1-11e7-8169-525400c83acfparallelism:1completions:1start :sat, 25 mar 2017 20:05:41 -0700labels:controller-uid=18a6678e-11d1-11e7-8169-525400c83acfjob-name=badpods statuses:1 running / 0 succeeded / 24 failed volumes.events: firstseenlastseencountsubobjectpathtypereasonmessage ------------------------------------------------------------ 1m1m1{job-controller }normalsuccessfulcreatecreated pod: bad-fws8g 1m1m1{job-controller }normalsuccessfulcreatecreated pod: bad-321pk 1m1m1{job-controller }normalsuccessfulcreatecreated pod: bad-2pxq1 1m1m1{job-controller }normalsuccessfulcreatecreated pod: bad-kl2tj 1m1m1{job-controller }normalsuccessfulcreatecreated pod: bad-wfw8q 1m1m1{job-controller }normalsuccessfulcreatecreated pod: bad-lz0hq 1m1m1{job-controller }normalsuccessfulcreatecreated pod: bad-0dck0 1m1m1{job-controller }normalsuccessfulcreatecreated pod: bad-0lm8k 1m1m1{job-controller }normalsuccessfulcreatecreated pod: bad-q6ctf 1m1s16{job-controller }normalsuccessfulcreate(events common reason combined)probably the result expected. time, load the nodes docker be quite substantial,especially job is failing very quickly.let's clean the busy failing job :$ kubectl delete jobs/bad let's activedeadlineseconds limit amount retries:apiversion: batch/v1kind: jobmetadata: name: boundspec: activedeadlineseconds: 10 template: metadata: name: bound spec: restartpolicy: never containers: - name: box image: busybox command: [\"/bin/sh\", \"-c\", \"exit 1\"]$ kubectl create -f bound.yaml you see after 10 seconds, job has failed: 11s11s1{job-controller }normaldeadlineexceededjob was active longer specified deadlinenote: sometimes makes sense retry forever. this case sure set proper pod restart policy protect accidental ddos your cluster.production pattern: circuit breaker this example, web application is imaginary web server email. render page, frontend has make requests the backend:talk the weather service get current weatherfetch current mail the database the weather service is down, user still like review email, weather serviceis auxilliary, while current mail service is critical.here is frontend, weather mail services written python:weather flask import flaskapp = flask(__name__)@app.route(\"/\")def hello(): return '''pleasanton, casaturday 8:00 pmpartly cloudy12 cprecipitation: 9%humidity: 74%wind: 14 km/h''' __name__ == \"__main__\": app.run(host='0.0.0.0')mail flask import flask,jsonifyapp = flask(__name__)@app.route(\"/\")def hello(): return jsonify([ {\"\": \"<bob@example.com>\", \"subject\": \"lunch noon tomorrow\"}, {\"\": \"<alice@example.com>\", \"subject\": \"compiler docs\"}]) __name__ == \"__main__\": app.run(host='0.0.0.0')frontend flask import flaskimport requests datetime import datetimeapp = flask(__name__)@app.route(\"/\")def hello(): weather = \"weather unavailable\" try: print \"requesting weather...\" start = datetime.() r = requests.('http://weather') print \"got weather %s ...\" % (datetime.() - start) r.status_code == requests.codes.ok: weather = r.text except: print \"weather unavailable\" print \"requesting mail...\" r = requests.('http://mail') mail = r.json() print \"got mail %s ...\" % (datetime.() - start) = [] for letter mail: .append(\"<li>: %s subject: %s</li>\" % (letter[''], letter['subject'])) return '''<html><body> <h3>weather</h3> <p>%s</p> <h3>email</h3> <p> <ul> %s </ul> </p></body>''' % (weather, '<br/>'.join())if __name__ == \"__main__\": app.run(host='0.0.0.0')let's create deployments services:$ cd prod/cbreaker$ docker build -t $(minikube ip):5000/mail:0.0.1 .$ docker push $(minikube ip):5000/mail:0.0.1$ kubectl apply -f service.yamldeployment \"frontend\" configureddeployment \"weather\" configureddeployment \"mail\" configuredservice \"frontend\" configuredservice \"mail\" configuredservice \"weather\" configuredcheck everyting is running smoothly:$ kubectl run - -t --rm cli --image=tutum/curl --restart=never$ curl http://frontend<html><body> <h3>weather</h3> <p>pleasanton, casaturday 8:00 pmpartly cloudy12 cprecipitation: 9%humidity: 74%wind: 14 km/h</p> <h3>email</h3> <p> <ul> <li>: <bob@example.com> subject: lunch noon tomorrow</li><br/><li>: <alice@example.com> subject: compiler docs</li> </ul> </p></body>let's introduce weather service crashes: flask import flaskapp = flask(__name__)@app.route(\"/\")def hello(): raise exception(\" am of service\") __name__ == \"__main__\": app.run(host='0.0.0.0')build redeploy:$ docker build -t $(minikube ip):5000/weather-crash:0.0.1 -f weather-crash.dockerfile .$ docker push $(minikube ip):5000/weather-crash:0.0.1$ kubectl apply -f weather-crash.yaml deployment \"weather\" configuredlet's sure it is crashing:$ kubectl run - -t --rm cli --image=tutum/curl --restart=never$ curl http://weather<!doctype html public \"-//w3c//dtd html 3.2 final//en\"><title>500 internal server error</title><h1>internal server error</h1><p> server encountered internal error was unable complete request. either server is overloaded there is error the application.</p>however frontend should all :$ kubectl run - -t --rm cli --image=tutum/curl --restart=nevercurl http://frontend<html><body> <h3>weather</h3> <p>weather unavailable</p> <h3>email</h3> <p> <ul> <li>: <bob@example.com> subject: lunch noon tomorrow</li><br/><li>: <alice@example.com> subject: compiler docs</li> </ul> </p></body>root@cli:/# curl http://frontend <html><body> <h3>weather</h3> <p>weather unavailable</p> <h3>email</h3> <p> <ul> <li>: <bob@example.com> subject: lunch noon tomorrow</li><br/><li>: <alice@example.com> subject: compiler docs</li> </ul> </p></body>everything is working expected! is problem though, have observed service is crashing quickly, let's what happens our weather service is slow. happens more often production, e.g. due network database overload. simulate failure are going introduce artificial delay: flask import flaskimport app = flask(__name__)@app.route(\"/\")def hello(): .sleep(30) raise exception(\"system overloaded\") __name__ == \"__main__\": app.run(host='0.0.0.0')build redeploy:$ docker build -t $(minikube ip):5000/weather-crash-slow:0.0.1 -f weather-crash-slow.dockerfile .$ docker push $(minikube ip):5000/weather-crash-slow:0.0.1$ kubectl apply -f weather-crash-slow.yaml deployment \"weather\" configured as expected, weather service is timing :curl http://weather <!doctype html public \"-//w3c//dtd html 3.2 final//en\"><title>500 internal server error</title><h1>internal server error</h1><p> server encountered internal error was unable complete request. either server is overloaded there is error the application.</p> problem though, is every request frontend takes 10 seconds wellcurl http://frontend is much more common outage - users leave frustration the service is unavailable. fix issue are going introduce special proxy circuit breaker.circuit breaker is special middleware is designed provide fail- action case service has degraded. is very helpful prevent cascading failures - where failure the service leads failure another. circuit breaker observes requests statistics checks stats against special error condition.here is simple circuit breaker written python: flask import flaskimport requests datetime import datetime, timedelta threading import lockimport logging, sysapp = flask(__name__)circuit_tripped_until = datetime.()mutex = lock()def trip(): global circuit_tripped_until mutex.acquire() try: circuit_tripped_until = datetime.() + timedelta(0,30) app.logger.info(\"circuit tripped until %s\" %(circuit_tripped_until)) finally: mutex.release()def is_tripped(): global circuit_tripped_until mutex.acquire() try: return datetime.() < circuit_tripped_until finally: mutex.release() @app.route(\"/\")def hello(): weather = \"weather unavailable\" try: is_tripped(): return \"circuit breaker: service unavailable (tripped)\" r = requests.('http://localhost:5000', timeout=1) app.logger.info(\"requesting weather...\") start = datetime.() app.logger.info(\"got weather %s ...\" % (datetime.() - start)) r.status_code == requests.codes.ok: return r.text else: trip() return \"circuit breaker: service unavailable (tripping 1)\" except: app.logger.info(\"exception: %s\", sys.exc_info()[0]) trip() return \"circuit breaker: service unavailable (tripping 2)\" __name__ == \"__main__\": app.logger.addhandler(logging.streamhandler(sys.stdout)) app.logger.setlevel(logging.debug) app.run(host='0.0.0.0', port=6000)let's build redeploy circuit breaker:$ docker build -t $(minikube ip):5000/cbreaker:0.0.1 -f cbreaker.dockerfile .$ docker push $(minikube ip):5000/cbreaker:0.0.1$ kubectl apply -f weather-cbreaker.yaml deployment \"weather\" configured$ kubectl apply -f weather-service.yamlservice \"weather\" configuredcircuit breaker detect service outage auxilliary weather service not bring mail service down more:curl http://frontend<html><body> <h3>weather</h3> <p>circuit breaker: service unavailable (tripped)</p> <h3>email</h3> <p> <ul> <li>: <bob@example.com> subject: lunch noon tomorrow</li><br/><li>: <alice@example.com> subject: compiler docs</li> </ul> </p></body>note: are production level proxies natively support circuit breaker pattern - vulcand nginx plusproduction pattern: sidecar rate connection limiting the previous example have used sidecar pattern - special proxy local the pod, adds additional logic the service, such error detection, tls termination other features.here is example sidecar nginx proxy adds rate connection limits:$ cd prod/sidecar$ docker build -t $(minikube ip):5000/sidecar:0.0.1 -f sidecar.dockerfile .$ docker push $(minikube ip):5000/sidecar:0.0.1$ docker build -t $(minikube ip):5000/service:0.0.1 -f service.dockerfile .$ docker push $(minikube ip):5000/service:0.0.1$ kubectl apply -f sidecar.yamldeployment \"sidecar\" configuredtry hit service faster one request per second you see rate limiting action$ kubectl run - -t --rm cli --image=tutum/curl --restart=nevercurl http://sidecar"
}