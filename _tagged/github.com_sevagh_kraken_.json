{
	"_id": "14406793",
	"site": "https://github.com/sevagh/kraken/",
	"title": " Attach and mount EBS volumes from EC2 instances based on tags",
	"author": "sevagh",
	"date": "2017-06-13T13:06:45.846Z",
	"tags": {
		"categories": [
			"opensource",
			"ebs-volumes",
			"raid",
			"ec2",
			"aws",
			"golang",
			"mdadm"
		],
		"languages": [
			"go",
			"makefile"
		]
	},
	"content": "readme.md very early alpha - at own riskgoat attach & mount ebs volumes raid arrays inside ec2 instancesgoat is go application runs inside ec2 instance ('s necessary the instance have iam role full ec2 access). setting tags correctly, goat discover, attach, raid ( mdadm), mkfs, mount ebs volumes the ec2 instance where 's running.usageput in ec2 user-data ( at script runs boot):$ yum install -y wget mdadm$ wget https://github.com/sevagh/goat/releases/download/0.2.0/goat$ chmod +x ./goat$ ./goat >/var/log/goat.log 2>&1behaviorgoat should behave correctly no parameters. is configured entirely tags (explained below). logs stderr default. takes options:--dry - dry run, don't execute commands--log-level=<level> - logrus log levels (.e. debug, info, warn, error, fatal, panic)--debug - interactive debug mode prompts continue every phase you explore state between phasesfresh run event flow is roughly following: ec2 metadata the running instance metadata establish ec2 client scan ebs volumesattach volumes needs based their tags typical linux disk tools mount drives correctly:mdadm raid volumes ( needed)blkid check an existing filesystemmkfs make filesystem/etc/fstab entries preserve behavior reboot filesystem fstab entries are created the label goat-{volumename} convenience. running goat multiple times result it detecting existing label intended create not proceeding.attaching old disks a instance specific -case goat was developed solve is following. you 3 instances their own respective disks, you receive termination notice instance 1. want goat workflow be:terminate instance 1create new with same goat tags ( indicate it's same the machine is replacing)everything works magically goat ran the previous instance fresh disks, disks the correct filesystems, labels, in case raid, mdadm metadata them. event flow a re-created instance ( disks were previously attached another instance) is: ec2 metadata metadata establish ec2 client scan ebs volumesattach volumes needs based their tagsdiscover /dev/disk/-label already contains correct disks mdadm magic, the ebs attachment raid array is already detected correctlyproceed perform fstab mount phases - skip mdadm, mkfscaveat: mdadm metadata have hostname the previous ec2 instance:[centos@ip-172-31-29-69 ~]$ sudo mdadm --detail --scan --verbosearray /dev/md127 level=raid0 num-devices=3 metadata=1.2 name=\"ip-172-31-25-105:'goat-data'\" uuid=2d08b310:fd13bd21:bc2417a4:56a1ec57 devices=/dev/xvdb,/dev/xvdc,/dev/xvdd[centos@ip-172-31-29-69 ~]$ avoid , define good/persistent hostname ec2 instance, you then re-apply any instance taking this instance's disks.run phase production run goat the ec2 user-data phase (executed a bash script). further exploration is needed perhaps embed properly systemd cloud-init.tags are tags need:tag namedescriptionec2ebstag value (examples)goat-:prefixlogical stack nameyesyesmy_app_v1.3.4goat-:nodeidec2 id within stackyesyes0, 1, 2 3-node kafkagoat-:volumenamedistinct volume nameyesdata, log - raid disks must the same volumenamegoat-:volumesize# disks vol groupyes2 2-disk raid, 1 single disk/ raidgoat-:raidlevellevel raid (0 1)yes0 1 raid, ignored volumesize == 1goat-:mountpathlinux path mount volyes/var/kafka_datagoat-:fstypelinux filesystem typeyesext4, vfatexampleslink the example terraform hcl scripts. , the gitpitch presentation has partial demonstration.here's example clarify better.2-instance vertica cluster:ec2 instance:goat-:prefix: adgear_vertica_v0.5goat-:nodeid: 0ec2 instance:goat-:prefix: adgear_vertica_v0.5goat-:nodeid: 1 want 2 raid0 volumes (2 disks each), 4 total disks (2 per instance):2x ebs volume node 0:goat-:prefix: adgear_vertica_v0.5goat-:nodeid: 0goat-:volumename: vdatagoat-:volumesize: 2goat-:raidlevel: 0goat-:mountpath: /vertica/datagoat-:fstype: ext42x ebs volume node 1:goat-:prefix: adgear_vertica_v0.5goat-:nodeid: 1goat-:volumename: vdatagoat-:volumesize: 2goat-:raidlevel: 0goat-:mountpath: /vertica/datagoat-:fstype: ext4additionally, want 1 extra disk (single disks, raid) per node, logs:1x ebs volume node 0:goat-:prefix: adgear_vertica_v0.5goat-:nodeid: 0goat-:volumename: vloggoat-:volumesize: 1goat-:raidlevel: 0goat-:mountpath: /vertica/loggoat-:fstype: ext41x ebs volume node 1:goat-:prefix: adgear_vertica_v0.5goat-:nodeid: 1goat-:volumename: vloggoat-:volumesize: 1goat-:raidlevel: 0goat-:mountpath: /vertica/loggoat-:fstype: ext4run goat the ec2 instance (ideally the user-data phase) automatically mount associated ebs volumes the above properties:[dbadmin@ip-172-31-46-84 ~]$ ls /dev/disk/-label/goat-vdata goat-vlog[dbadmin@ip-172-31-46-84 ~]$[dbadmin@ip-172-31-46-84 ~]$[dbadmin@ip-172-31-46-84 ~]$ tail -n1 /etc/fstablabel=goat-log /vertica/log ext4 defaults 0 1label=goat-vdata /vertica/data ext4 defaults 0 1[dbadmin@ip-172-31-46-84 ~]$ sudo mdadm /dev/md0... name : \"goat-vdata\" uuid : ddf766d8:ab73e885:e6e35b57:3dc7eb28 events : 0...[dbadmin@ip-172-31-46-84 ~]$ df -h | tail -n2/dev/xvdc 20g 45m 19g 1% /vertica/log/dev/md0 222g 61m 210g 1% /vertica/datamotivation terraform resource aws_volume_attachment isn't handled when destroying stack. here some discussion the matter. initially wrote instance-specific user-data shell scripts hardcoded values (e.g. mkfs.ext4 /dev/xvdb, mount /dev/xvdb /var/kafka_data). goat can avoid needing pass parameters hardcoding values. the required information comes the ec2 instance ebs volume tags."
}