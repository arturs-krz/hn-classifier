{
	"_id": "14375620",
	"site": "https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/data/README.md",
	"title": "Using the Dataset API for TensorFlow Input Pipelines",
	"author": "mrry",
	"date": "2017-06-13T13:57:07.560Z",
	"tags": {
		"categories": [
			"opensource"
		],
		"languages": []
	},
	"content": "using dataset api tensorflow input pipelines dataset api is designed let build complex input pipelines simple, reusable pieces. example, pipeline an image model mightaggregate data files a distributed file system, apply randomperturbations each image, merge randomly selected images a batch training. pipeline a text model might involve extracting symbols raw text data, converting to embedding identifiers a lookuptable, batching together sequences different lengths. dataset apimakes easy deal large amounts data, different data formats, complicated transformations. dataset api introduces new abstractions tensorflow: tf.contrib.data.dataset represents sequence elements, whicheach element contains or more tensor objects. example, an imagepipeline, element might a single training example, a pair tensors representing image data a label. dataset either asource (e.g. dataset.from_tensor_slices() constructs dataset one more tf.tensor objects), a transformation (e.g. dataset.batch()constructs dataset stacking consecutive elements another dataset a single element). tf.contrib.data.iterator provides main to extract elements adataset. iterator.get_next() operation yields next element adataset, typically acts the interface between input pipeline code your model. simplest iterator is \"one-shot iterator\", isassociated a particular dataset iterates through once. moresophisticated uses, iterator.initializer operation enables toreinitialize parameterize iterator different datasets, that can, example, iterate training validation data multiple times the same program.tutorial programmers' guide includes step--step instructions a variety input data cases. see dataset iterator class references more detailed information the api.basic mechanics section the guide describes fundamentals creating different kinds dataset iterator objects, how extract data them.defining source dataset can build dataset using of following source datasetconstructors: in-memory data:tf.contrib.data.dataset.from_tensors()tf.contrib.data.dataset.from_tensor_slices() on-disk data:tf.contrib.data.fixedlengthrecorddataset()tf.contrib.data.textlinedataset()tf.contrib.data.tfrecorddataset() parameters:tf.contrib.data.dataset.range()transforming dataset tf.contrib.data.dataset class has many methods can chainedtogether transform dataset another:per-element transformations:dataset.filter()dataset.flat_map()dataset.map()dataset.zip()multi-element transformations:dataset.batch()dataset.dense_to_sparse_batch()dataset.group_by_window()dataset.padded_batch()dataset.repeat()dataset.shuffle()dataset.skip()dataset.()the following sections contain examples how use transformations solve common problems.dataset structure dataset comprises elements each the same structure. elementcontains or more tf.tensor objects, called components. each componenthas tf.dtype representing type elements the tensor, atf.tensorshape representing (possibly partially specified) static shape each element. dataset.output_types dataset.output_shapes propertiesallow to inspect inferred types shapes each component adataset element. nested structure these properties map the structure an element, may a single tensor, tuple tensors, a nestedtuple tensors. example:dataset1 = tf.contrib.data.dataset.from_tensor_slices(tf.random_uniform([4, 10]))print(dataset1.output_types) # ==> \"tf.float32\"print(dataset1.output_shapes) # ==> \"(10,)\"dataset2 = tf.contrib.data.dataset.from_tensor_slices( (tf.random_uniform([4]), tf.random_uniform([4, 100], maxval=100, dtype=tf.int32)))print(dataset2.output_types) # ==> \"(tf.float32, tf.int32)\"print(dataset2.output_shapes) # ==> \"((), (100,))\"dataset3 = tf.contrib.data.dataset.zip((dataset1, dataset2))print(dataset3.output_types) # ==> (tf.float32, (tf.float32, tf.int32))print(dataset3.output_shapes) # ==> \"(10, ((), (100,)))\" dataset transformations support datasets any structure. using dataset.map(), dataset.flat_map() dataset.filter() transformations, apply function each element, element structure determines arguments the function:dataset1 = dataset1.map(lambda x: ...)dataset2 = dataset2.flat_map(lambda x, y: ...)# *n.b.* lambda argument destructuring is available python 3.dataset3 = dataset3.filter(lambda x, (y, z): ...)creating iterator you built dataset represent input data, next step is create iterator access elements that dataset. dataset apicurrently supports three kinds iterator, increasing level sophistication: one-shot iterator is simplest form iterator, only supportsiterating once through dataset, no need explicit initialization.-shot iterators handle almost of cases the existing queue-basedinput pipelines support, they not support parameterization. using example dataset.range():dataset = tf.contrib.data.dataset.range(100)iterator = dataset.make_one_shot_iterator()next_element = iterator.get_next() i range(100): value = sess.run(next_element) assert == value initializable iterator requires to run explicititerator.initializer operation before using . in exchange thisinconvenience, enables to parameterize definition the dataset,using or more tf.placeholder() tensors can fed youinitialize iterator. continuing dataset.range() example:max_value = tf.placeholder(tf.int64, shape=[])dataset = tf.contrib.data.dataset.range(max_value)iterator = dataset.make_initializable_iterator()next_element = iterator.get_next()# initialize iterator a dataset 10 elements.sess.run(iterator.initializer, feed_dict={max_value: 10}) i range(10): value = sess.run(next_element) assert == value# initialize same iterator a dataset 100 elements.sess.run(iterator.initializer, feed_dict={max_value: 100}) i range(100): value = sess.run(next_element) assert == value reinitializable iterator be initialized multiple differentdataset objects. example, might a training input pipeline uses random perturbations the input images improve generalization, a validation input pipeline evaluates predictions unmodified data. pipelines typically different dataset objects have samestructure (.e. same types compatible shapes each component).training_dataset = tf.contrib.data.dataset.range(100).map( lambda x: x + tf.random_uniform([], -10, 10, tf.int64))validation_dataset = tf.contrib.data.dataset.range(50)# reinitializable iterator is defined its structure. could the# `output_types` `output_shapes` properties either `training_dataset`# `validation_dataset` here, they are compatible.iterator = iterator.from_structure(training_dataset.output_types, training_dataset.output_shapes)next_element = iterator.get_next()training_init_op = iterator.make_initializer(training_dataset)validation_init_op = iterator.make_initializer(validation_dataset)# run 20 epochs which training dataset is traversed, followed the# validation dataset. _ range(20): # initialize iterator the training dataset. sess.run(training_init_op) _ range(100): sess.run(next_element) # initialize iterator the validation dataset. sess.run(validation_init_op) _ range(50): sess.run(next_element)consuming values an iterator iterator.get_next() method returns or more tf.tensor objects correspond the symbolic next element an iterator. each these tensorsare evaluated, take value the next element the underlyingdataset. (note , like stateful objects tensorflow, callingiterator.get_next() does immediately advance iterator. instead must the returned tf.tensor objects a tensorflow expression, pass result that expression tf.session.run() get next elements advance iterator.) the iterator reaches end the dataset, executing iterator.get_next() operation raise tf.errors.outofrangeerror. this point iterator be an unusable state, you mustinitialize again you to it further.dataset = tf.contrib.data.dataset.range(5)iterator = dataset.make_initializable_iterator()next_element = iterator.get_next()# typically `result` be output a model, an optimizer's# training operation.result = tf.add(next_element, next_element)sess.run(iterator.initializer)print(sess.run(result)) # ==> \"0\"print(sess.run(result)) # ==> \"2\"print(sess.run(result)) # ==> \"4\"print(sess.run(result)) # ==> \"6\"print(sess.run(result)) # ==> \"8\"try: sess.run(result)except tf.errors.outofrangeerror: print(\"end dataset\") # ==> \"end dataset\" common pattern is wrap \"training loop\" a try-except block:sess.run(iterator.initializer)while true: try: sess.run(result) except tf.errors.outofrangeerror: break each element the dataset has nested structure, return value iterator.get_next() be or more tf.tensor objects the samenested structure:dataset1 = tf.contrib.data.dataset.from_tensor_slices(tf.random_uniform([4, 10]))dataset2 = tf.contrib.data.dataset.from_tensor_slices((tf.random_uniform([4]), tf.random_uniform([4, 100])))dataset3 = tf.contrib.data.dataset.zip((dataset1, dataset2))iterator = dataset3.make_initializable_iterator()sess.run(iterator.initializer)next1, (next2, next3) = iterator.get_next()note evaluating of next1, next2, next3 advance iterator all components. typical consumer an iterator include components a single expression.reading input dataconsuming numpy arrays all your input data fit memory, simplest to create dataset them is convert to tf.tensor objects usedataset.from_tensor_slices().# load training data two numpy arrays, example using `np.load()`. np.load(\"/var/data/training_data.npy\") data: features = data[\"features\"] labels = data[\"labels\"]# assume each row `features` corresponds the same row `labels`.assert features.shape[0] == labels.shape[0]dataset = tf.contrib.data.dataset.from_tensor_slices((features, labels))note the above code snippet embed features labels arrays your tensorflow graph constants. works for small dataset, wastes memory, can run the 2gb limit the tf.graphdef protocolbuffer. an alternative, can define dataset terms tf.placeholder()tensors, feed numpy arrays you initialize iterator thedataset.# load training data two numpy arrays, example using `np.load()`. np.load(\"/var/data/training_data.npy\") data: features = data[\"features\"] labels = data[\"labels\"]# assume each row `features` corresponds the same row `labels`.assert features.shape[0] == labels.shape[0]features_placeholder = tf.placeholder(features.dtype, features.shape)labels_placeholder = tf.placeholder(labels.dtype, labels.shape)dataset = tf.contrib.data.dataset.from_tensor_slices((features_placeholder, labels_placeholder))# [ transformations `dataset`...]dataset = ...iterator = dataset.make_initializable_iterator()sess.run(iterator.initializer, feed_dict={features_placeholder: features, labels_placeholder: labels})consuming tfrecord data dataset api supports variety file formats that can processlarge datasets do fit memory. tfrecord file format is simple record-oriented binary format many tensorflow applications fortraining data. tf.contrib.data.tfrecorddataset class enables tostream the contents one more tfrecord files part an inputpipeline.# creates dataset reads of examples two files.filenames = [\"/var/data/file1.tfrecord\", \"/var/data/file2.tfrecord\"]dataset = tf.contrib.data.tfrecorddataset(filenames) filenames argument the tfrecorddataset initializer be tf.tensor strings. therefore you two sets files training validation purposes, can a tf.placeholder(tf.string) represent filenames, initialize iterator the appropriate filenames:filenames = tf.placeholder(tf.string, shape=[none])dataset = tf.contrib.data.tfrecorddataset(filenames)# [ transformations `dataset`...]dataset = ...iterator = dataset.make_initializable_iterator()# can feed initializer the appropriate filenames the current# phase execution, e.g. training vs. validation.# initialize `iterator` training data.training_filenames = [\"/var/data/file1.tfrecord\", \"/var/data/file2.tfrecord\"]sess.run(iterator.initializer, feed_dict={filenames: training_filenames})# initialize `iterator` validation data.validation_filenames = [\"/var/data/validation1.tfrecord\", ...]sess.run(iterator.initializer, feed_dict={filenames: validation_filenames})consuming text datamany datasets are distributed one more text files. tf.contrib.data.textlinedataset provides easy to extract lines one more text files. given or more filenames, textlinedataset produce string-valued element per line those files. atfrecorddataset, textlinedataset accepts filenames a tf.tensor, you parameterize by passing tf.placeholder(tf.string).filenames = [\"/var/data/file1.txt\", \"/var/data/file2.txt\"]dataset = tf.contrib.data.textlinedataset(filenames) default, textlinedataset yields every line each file, may be desirable, example the file starts a header line, containscomments. lines be removed using dataset.skip() dataset.filter() transformations. apply transformations eachfile separately, use dataset.flat_map() create nested dataset each file.filenames = [\"/var/data/file1.txt\", \"/var/data/file2.txt\"]dataset = tf.contrib.data.dataset.from_tensor_slices(filenames)# `dataset.flat_map()` transform each file separately.# * skip first line (header row).# * filter lines beginning \"#\" (comments).dataset = dataset.flat_map( lambda filename: ( tf.contrib.data.dataset.textlinedataset(filename) .skip(1) .filter(lambda line: tf.not_equal(tf.substr(line, 0, 1), \"#\"))))preprocessing data dataset.map() dataset.map(f) transformation produces new dataset applying givenfunction f each element the input dataset. is based themap() function is commonly applied lists ( other structures) functionalprogramming languages. function f takes tf.tensor objects represent single element the input, returns tf.tensor objects will represent single element the dataset. implementation usesstandard tensorflow operations transform element another. section covers common examples how use dataset.map().parsing tf.example protocol buffer messagesmany input pipelines extract tf.train.example protocol buffer messages atfrecord-format file (written, example, usingtf.python_io.tfrecordwriter). each tf.train.example record contains ormore \"features\", the input pipeline typically converts features tensors.# transforms scalar string `example_proto` a pair a scalar string # a scalar integer, representing image its label, respectively.def _parse_function(example_proto): features = {\"image\": tf.fixedlenfeature((), tf.string, default_value=\"\"), \"label\": tf.fixedlenfeature((), tf.int32, default_value=0)} parsed_features = tf.parse_single_example(example_proto, features) return parsed_features[\"image\"], parsed_features[\"label\"]# creates dataset reads of examples two files, extracts# image label features.filenames = [\"/var/data/file1.tfrecord\", \"/var/data/file2.tfrecord\"]dataset = tf.contrib.data.tfrecorddataset(filenames)dataset = dataset.map(_parse_function)decoding image data resizing when training neural network real-world image data, is often necessary convert images different sizes a common size, that may batched a fixed size.# reads image a file, decodes into dense tensor, resizes # to fixed shape.def _parse_function(filename, label): image_string = tf.read_file(filename) image_decoded = tf.image.decode_image(image_string) image_resized = tf.image.resize_images(image_decoded, [28, 28]) return image_resized, labelfilenames = tf.constant([\"/var/data/image1.jpg\", \"/var/data/image2.jpg\", ...])labels = tf.constant([0, 37, 29, 1, ...])dataset = tf.contrib.data.dataset.from_tensor_slices((filenames, labels))dataset = dataset.map(_parse_function)applying arbitrary python logic tf.py_func() performance reasons, encourage to tensorflow operations preprocessing data whenever possible. however, is sometimes useful be able call upon external python libraries parsing input data, you do by invoking tf.py_func() operation adataset.map() transformation.import cv2# a custom opencv function read image, instead the standard# tensorflow `tf.read_file()` operation.def _read_py_function(filename, label): image_decoded = cv2.imread(image_string, cv2.imread_grayscale) return image_decoded, label# standard tensorflow operations resize image a fixed shape.def _resize_function(image_decoded, label): image_decoded.set_shape([none, none, none]) image_resized = tf.image.resize_images(image_decoded, [28, 28]) return image_resized, labelfilenames = [\"/var/data/image1.jpg\", \"/var/data/image2.jpg\", ...]labels = [0, 37, 29, 1, ...]dataset = tf.contrib.data.dataset.from_tensor_slices((filenames, labels))dataset = dataset.map( lambda filename, label: tf.py_func( _read_py_function, [filename, label], [tf.uint8, label.dtype]))dataset = dataset.map(_resize_function)batching dataset elementssimple batching simplest form batching stacks n consecutive elements a dataset a single element. dataset.batch() transformation does exactly , with same constraints the tf.stack() operator, applied each component the elements: .e. each component , all elements must a tensor the exact same shape.inc_dataset = tf.contrib.data.dataset.range(100)dec_dataset = tf.contrib.data.dataset.range(0, -100, -1)dataset = tf.contrib.data.dataset.zip((inc_dataset, dec_dataset))batched_dataset = dataset.batch(4)iterator = batched_dataset.make_one_shot_iterator()next_element = iterator.get_next()print(sess.run(next_element)) # ==> ([0, 1, 2, 3], [ 0, -1, -2, -3])print(sess.run(next_element)) # ==> ([4, 5, 6, 7], [-4, -5, -6, -7])print(sess.run(next_element)) # ==> ([8, 9, 10, 11], [-8, -9, -10, -11])batching tensors padding above recipe works tensors all the same size. however, manymodels (e.g. sequence models) with input data can varying size(e.g. sequences different lengths). handle case, dataset.padded_batch() transformation enables to batch tensors different shape specifying or more dimensions which may padded.dataset = tf.contrib.data.dataset.range(100)dataset = dataset.map(lambda x: tf.fill([tf.cast(x, tf.int32)], x))dataset = dataset.padded_batch(4, padded_shapes=[none])iterator = batched_dataset.make_one_shot_iterator()next_element = iterator.get_next()print(sess.run(next_element)) # ==> [[0, 0, 0], [1, 0, 0], [2, 2, 0], [3, 3, 3]]print(sess.run(next_element)) # ==> [[4, 4, 4, 4, 0, 0, 0], # [5, 5, 5, 5, 5, 0, 0], # [6, 6, 6, 6, 6, 6, 0], # [7, 7, 7, 7, 7, 7, 7]] dataset.padded_batch() transformation allows to set different padding each dimension each component, it may variable-length (signified none the example above) constant-length. is possible override padding value, defaults 0.training workflowsprocessing multiple epochs dataset api offers main ways process multiple epochs the samedata. simplest to iterate a dataset multiple epochs is use dataset.repeat() transformation. example, create dataset repeats input 10 epochs:filenames = [\"/var/data/file1.tfrecord\", \"/var/data/file2.tfrecord\"]dataset = tf.contrib.data.tfrecorddataset(filenames)dataset = dataset.map(...)dataset = dataset.repeat(10)dataset = dataset.batch(32)applying dataset.repeat() transformation no arguments repeat input indefinitely. dataset.repeat() transformation concatenates arguments without signaling end one epoch the beginning the nextepoch. you to receive signal the end each epoch, can write training loop catches tf.errors.outofrangeerror the end adataset. that point might collect statistics (e.g. validationerror) the epoch.filenames = [\"/var/data/file1.tfrecord\", \"/var/data/file2.tfrecord\"]dataset = tf.contrib.data.tfrecorddataset(filenames)dataset = dataset.map(...)dataset = dataset.batch(32)iterator = dataset.make_initializable_iterator()next_element = iterator.get_next()# compute 100 epochs. _ range(100): sess.run(iterator.initializer) while true: try: sess.run(next_element) except tf.errors.outofrangeerror: break # [perform end--epoch calculations here.]randomly shuffling input data dataset.shuffle() transformation randomly shuffles input datasetusing similar algorithm tf.randomshufflequeue: maintains fixed-sizebuffer chooses next element uniformly random that buffer.filenames = [\"/var/data/file1.tfrecord\", \"/var/data/file2.tfrecord\"]dataset = tf.contrib.data.tfrecorddataset(filenames)dataset = dataset.map(...)dataset = dataset.repeat()dataset = dataset.shuffle(buffer_size=10000)dataset = dataset.batch(32)"
}