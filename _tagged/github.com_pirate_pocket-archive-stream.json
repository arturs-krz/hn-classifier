{
	"_id": "14272133",
	"site": "https://github.com/pirate/pocket-archive-stream",
	"title": " Pocket Stream Archive â€“ A personal Way-Back Machine",
	"author": "nikisweeting",
	"date": "2017-06-13T13:32:03.508Z",
	"tags": {
		"categories": [
			"opensource",
			"pocket",
			"archive",
			"wget",
			"google-chrome",
			"rss"
		],
		"languages": [
			"python",
			"html"
		]
	},
	"content": "readme.md pocket/pinboard/browser bookmark archiver ( own personal -back machine) demo: sweeting./pocketsave archived copy all websites star using pocket, pinboard, browser bookmarks.outputs browsable html archives each site, pdf, screenshot, a link a copy archive.org, indexed a nice html file.(powered the headless google chrome good 'ol wget.): also submits each link save archive.org!quickstart./archive.py link_export.html [pocket|pinboard|bookmarks]archive.py is script takes pocket, pinboard, browser bookmark html export file, turns into browsable archive you store locally host online.1. install dependencies: google-chrome >= 59,wget >= 1.16, python3 >= 3.5 (chromium >= v59 works , yay open source!)# mac:brew install caskroom/versions/google-chrome-canary wget python3echo -e '#!/bin/bash/applications/google\\ chrome\\ canary.app/contents/macos/google\\ chrome\\ canary \"$@\"' > /usr/local/bin/google-chromechmod +x /usr/local/bin/google-chrome# linux:wget -q -o - https://dl-ssl.google.com/linux/linux_signing_key.pub | sudo apt-key add -sudo sh -c 'echo \"deb [arch=amd64] http://dl.google.com/linux/chrome/deb/ stable main\" >> /etc/apt/sources.list.d/google-chrome.list'apt update; apt install google-chrome-beta python3 wget# check:google-chrome --version && wget && python3 && echo \"[] dependencies installed.\"2. run archive script: your html export file pocket, pinboard, chrome bookmarks, firefox bookmarks, safari bookmarksclone repo git clone https://github.com/pirate/pocket-archive-streamcd pocket-archive-stream/./archive.py ~/downloads/exported_file.html [pocket|pinboard|bookmarks] produces folder pocket/ containing index.html, archived copies all sites,organized starred timestamp. each sites saves:wget site, e.g. en.wikipedia.org/wiki/example.html .html appended not presentsreenshot.png 1440x900 screenshot site using headless chromeoutput.pdf printed pdf site using headless chromearchive.org.txt link the saved site archive.org can tweak parameters screenshot size, file paths, timeouts, dependencies, the top archive.py. can tweak outputted html index index_template.html. just uses pythonformat strings ( a proper templating engine jinja2), is why css is double-bracketed {{...}}.estimated runtime: 've found takes an hour download 1000 articles, they'll up roughly 1gb.those numbers are running single-threaded my i5 machine 50mbps down. ymmv.troubleshooting: some linux distributions python3 package might be recent enough. this is case you, resort installing recent enough version manually.add-apt-repository ppa:fkrull/deadsnakes && apt update && apt install python3.6 you still need help, official python docs are good place start. switch google chrome chromium, change chrome_binary variable the top archive.py. you're missing wget curl, simply install using apt your package manager choice.live updating: (coming soon... maybe...)'s possible pull links via pocket api public pocket rss feeds instead downloading html export.once write script do , we stick in cron have auto-update it's own. now just to download ril_export.html run archive.py each it updates. script run fast subsequent times it downloads links haven't been archived already.publishing archive archive is suitable serving your personal server, can upload archive /var/www/pocket allow to access saved copies sites. stick in nginx config properly serve wget-archived sites:location /pocket/ { alias /var/www/pocket/; index index.html; autoindex ; try_files $uri $uri/ $uri.html =404;} sure 're running content cgi php, only to serve static files!urls like: https://sweeting./archive/archive/1493350273/en.wikipedia.org/wiki/dining_philosophers_probleminfo is basically open-source version pocket premium ( you should consider paying !).i got tired sites saved going offline changing urls, i startedarchiving copy them locally , similar the -back machine provided archive.org. self hosting own archive allows to savepdfs & screenshots dynamic sites addition static html, something archive.org doesn't .now can rest soundly knowing important articles resources like wont dissapear off internet. published archive an example: sweeting./pocket.security warning & content disclaimerhosting people's site content has security implications your domain, sure understand dangers hosting people's css & js files your domain. 's best put on domain its own slightly mitigate csrf attacks. may want blacklist archive your /robots.txt that search engines dont index content your domain. aware some sites archive may allow to rehost content publicly copyright reasons,'s to to host responsibly respond takedown requests appropriately.todobody text extraction using fathomauto-tagging based important extracted wordsaudio & video archiving youtube-dlfull-text indexing elasticsearchvideo closed-caption downloading full-text indexing video contentautomatic text summaries article summarization libraryfeature image extractionhttp support ( my https- domain)try wgetting dead sites archive.org (https://github.com/hartator/wayback-machine-downloader)linkshacker news discussionreddit r/selfhosted discussionreddit r/datahoarder discussionhttps://wallabag.org + https://github.com/wallabag/wallabaghttps://webrecorder.io/https://github.com/ikreymer/webarchiveplayer#auto-load-warcs"
}