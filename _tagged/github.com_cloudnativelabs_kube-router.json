{
	"_id": "14254949",
	"site": "https://github.com/cloudnativelabs/kube-router",
	"title": " Kube-router: A distributed load balancer, firewal, router for Kubernetes",
	"author": "murali-reddy",
	"date": "2017-06-13T13:32:47.847Z",
	"tags": {
		"categories": [
			"opensource",
			"kubernetes",
			"kubernetes-networking",
			"kubernetes-service",
			"docker",
			"ipvs",
			"iptables",
			"gobgp"
		],
		"languages": [
			"go",
			"makefile"
		]
	},
	"content": "readme.md kube-routerkube-router is distributed load balancer, firewall router kubernetes. kube-router be configured provide each cluster node: ipvs/lvs based service proxy each node clusterip nodeport service types, providing service discovery load balancing ingress firewall the pods running the node per defined kubernetes network policies using iptables ipset bgp router advertise learn routes the pod ip's cross-node pod--pod connectivitywhy kube-router have kube-proxy provides service proxy load balancer. have several addons solutions flannel, calico, weave etc provide cross node pod--pod networking. simillarly are solutions calico enforce network policies. why we need kube-router a similar job? here is motivation: is challenging deploy, monitor troubleshoot multiple solutions runtime. independent solution need work together. kube-router aims provide operational simplicity combining the networking functionality can provided the node to cohesive solution. run kube-router daemonset, just running command kubectl create -f kube-router-daemonset.yaml have solution pod--pod networking, service proxy firewall each node.kube-router is motivated provide optimized solution performance. kube-router uses ipvs service proxy compared iptables kube-proxy. kube-router uses solutions ipsets optimize iptables rules matching while providing ingress firewall the pods. inter-node pod--pod communication, routing rules added kube-router ensures data path is efficient ( hop pod--pod connectivity) out overhead overlays.kube-router builds standard linux technologies, you verify configuration troubleshoot standard linux networking tools (ipvsadm, ip route, iptables, ipset, traceroute, tcpdump etc). it actiongetting startedbuilding version 1.7 above is required build kube-router the dependencies are vendored already, just run build go build -o kube-router kube-router. to buildalternatively can download prebuilt binary https://github.com/cloudnativelabs/kube-router/releasescommand line options --run-firewall false, kube-router won't setup iptables provide ingress firewall pods. true default. --run-router true each node advertise routes rest the nodes learn routes the pods. false default --run-service-proxy false, kube-router won't setup ipvs services proxy. true default. --cleanup-config true cleanup iptables rules, ipvs, ipset configuration exit. --masquerade- snat traffic cluster ip/node port. false default --cluster-cidr cidr range pods the cluster. specified external traffic the pods be masquraded --config-sync-period duration often configuration the apiserver is refreshed. must greater 0. (default 1m0s) --iptables-sync-period duration maximum interval how often iptables rules are refreshed (e.g. '5s', '1m'). must greater 0. (default 1m0s) --ipvs-sync-period duration maximum interval how often ipvs config is refreshed (e.g. '5s', '1m', '2h22m'). must greater 0. (default 1m0s) --kubeconfig string path kubeconfig file authorization information ( master location is set the master flag). --master string address the kubernetes api server (overrides value kubeconfig) --routes-sync-period duration maximum interval how often routes are advertised learned (e.g. '5s', '1m', '2h22m'). must greater 0. (default 1m0s) --advertise-cluster-ip true cluster ip be added the rib will advertised the peers. false default. --cluster-asn asn number under cluster nodes run ibgp --peer-asn asn number the bgp peer which cluster nodes advertise cluster ip node's pod cidr --peer-router ip address the external router which nodes peer advertise cluster ip pod cidr's --nodes-full-mesh enabled each node the cluster setup bgp peer rest the nodes. true default --hostname-override non-empty, string be used identification node name instead the actual hostname.try kube-router cluster installerskopsplease the steps deploy kubernetes cluster kube-router using kopsbootkubeplease the steps deploy kubernetes cluster kube-router using bootkubedeploymentdepending what functionality kube-router want use, multiple deployment options are possible. can the flags --run-firewall, --run-router, --run-service-proxy selectively enable required functionality kube-router. you choose run kube-router agent running each cluster node. alternativley can run kube-router pod each node through daemonset.requirementskube-router need access kubernetes api server get information pods, services, endpoints, network policies etc. very minimum information requires is details where access kubernetes api server. information be passed kube-router --master=http://192.168.1.99:8080/ kube-router --kubeconfig=<path kubeconfig file>. neither --master nor --kubeconfig option is specified kube-router look kubeconfig /var/lib/kube-router/kubeconfig location. you run kube-router agent the node, ipset package must installed each the nodes ( run daemonset, container image is prepackaged ipset) you choose use kube-router pod--pod network connectivity kubernetes controller manager need be configured allocate pod cidrs passing --allocate-node-cidrs=true flag providing cluster-cidr (.e. passing --cluster-cidr=10.1.0.0/16 e.g.) you choose run kube-router daemonset, both kube-apiserver kubelet must run --allow-privileged=true option you choose use kube-router pod--pod network connecitvity kubernetes cluster must configured use cni network plugins. each node cni conf file is expected be present /etc/cni/net.d/10-kuberouter.conf .bridge cni plugin host-local ipam should used. sample conf file can downloaded wget -o /etc/cni/net.d/10-kuberouter.conf https://raw.githubusercontent.com/cloudnativelabs/kube-router/master/cni/10-kuberouter.confrunning daemonset is quickest to deploy kube-router (dont forget ensure requirements). runkubectl apply -f https://raw.githubusercontent.com/cloudnativelabs/kube-router/master/daemonset/kube-router--service-daemonset.yamlabove run kube-router pod each node automatically. can change arguments the daemonset definition required suit needs. samples be found https://github.com/cloudnativelabs/kube-router/tree/master/daemonset different argument select set the services kube-router should run.running agent can choose run kube-router agent runnng each node. e.g you want kube-router provide ingress firewall the pods you start kube-router kube-router --master=http://192.168.1.99:8080/ --run-firewall=true --run-service-proxy=false --run-router=falsecleanup configuration can clean all configurations done ( ipvs, iptables, ip routes) kube-router the node running kube-router --cleanup-configtrying kube-router alternative kube-proxy you a kube-proxy use, want try kube-router for service proxy can kube-proxy --cleanup-iptablesfollowed kube-router --master=http://192.168.1.99:8080/ --run-service-proxy=true --run-firewall=false --run-router=false if want move to kube-proxy clean config done kube-router running kube-router --cleanup-config run kube-proxy the configuration have.theory operationkube-router be run a agent a pod (through daemonset) each node leverages standard linux technologies iptables, ipvs/lvs, ipset, iproute2service proxy load balancingrefer https://cloudnativelabs.github.io/post/2017-05-10-kube-network-service-proxy/ the design details demokube-router uses ipvs/lvs technology built linux provide l4 load balancing. each the kubernetes service clusterip nodeport type is configured ipvs virtual service. each service endpoint is configured real server the virtual service.standard ipvsadm tool be used verify configuration monitor active connections.below is example set services kubernetes the endpoints the services how got mapped the ipvs kube-routerkube-router watches kubernetes api server get updates the services, endpoints automatically syncs ipvsconfiguration reflect desired state services. kube-router uses ipvs masquerading mode uses round robin schedulingcurrently. source pod ip is preserved that appropriate network policies be applied.pod ingress firewallrefer https://cloudnativelabs.github.io/post/2017-05-1-kube-network-policies/ the detailed design detailskube-router provides implementation network policies semantics through use iptables, ipset conntrack. the pods a namespace 'defaultdeny' ingress isolation policy has ingress blocked. traffic matcheswhitelist rules specified the network policies are permitted reach pod. following set iptables rules chains the 'filter' table are used achieve network policies semantics.each pod running the node, needs ingress blocked default is matched forward output chains fliter table send pod specific firewall chain. below rules are added match various casestraffic getting switched between pods the same node through bridgetraffic getting routed between pods different nodestraffic originating a pod going through service proxy getting routed pod same nodeeach pod specific firewall chain has default rule block traffic. rules are added jump traffic the network policyspecific policy chains. rules cover policies apply the destination pod ip. rule is added accept the established traffic permit return traffic.each policy chain has rules expressed through source destination ipsets. set pods matching ingress rule network policy specforms source pod ip ipset. set pods matching pod selector ( destination pods) the network policy formsdestination pod ip ipset.finally ipsets are created are used forming rules the network policy specific chainkube-router runtime watches kubernetes api server changes the namespace, network policy pods dynamically updates iptables ipset configuration reflect desired state ingress firewall the pods.pod networkingplease the blog design details.kube-router is expected run each node. subnet the node is learnt kube-router the cni configuration file the node through node.podcidr. each kube-routerinstance the node acts bgp router advertise pod cidr assigned the node. each node peers rest thenodes the cluster forming full mesh. learned routes the pod cidr the nodes (bgp peers) are injected local node routing table. the data path, inter node pod--pod communication is done routing stack the node.todoconvert kube-router docker image run as daemonsetheathcheck podsexplore integration an ingress controller kube-router be complete solution both east-west north-south traffic pod cidr node.podcidr kube-controller-manager is run --allocate-node-cidrs=true optionexplore possibility using ipvs direct routing modeexplore possibilities making kube-router the node prometheus endpointsession persistenceacknowledgementkube-router build upon following libraries:iptables: https://github.com/coreos/-iptablesgobgp: https://github.com/osrg/gobgpnetlink: https://github.com/vishvananda/netlinkipset: https://github.com/janeczku/-ipsetipvs: https://github.com/mqliang/libipvsfeedbackkube-router is active development, most -to-date version is head. are many more things explore around ipvs monitoring. you experience problems, feel free leave feedback raise questions any by opening issue here."
}