{
	"_id": "14367532",
	"site": "https://github.com/pltlg/FCMNN",
	"title": "Multi-Layer Neural Network in Swift",
	"author": "pltlg",
	"date": "2017-06-13T13:57:07.975Z",
	"tags": {
		"categories": [
			"opensource"
		],
		"languages": [
			"swift"
		]
	},
	"content": "readme.md fcmnn library aims provide simplistic implementation a fully connected multilayered neural network swift language. to add? drag fcmnn.swift your project. simple that. demo project lives this repo.basic terms goal the neural network is solve problems the same that human , although several neural network categories are more abstract.feedforward feedforward neural network is artificial neural network wherein connections between units not form cycle. such, is different recurrent neural networks. feedforward neural network was first simplest type artificial neural network devised. this network, information moves only direction, forward, the input nodes, through hidden nodes ( any) to output nodes. are cycles loops the network.multi-layer class networks consists multiple layers computational units, usually interconnected a feed-forward . each neuron one layer has directed connections the neurons the subsequent layer. many applications units these networks apply sigmoid function an activation function.activation activation function converts neuron's weighted input its output activation.fully connected layers high-level reasoning the neural network is done via fully connected layers. neurons a fully connected layer full connections all activations the previous layer, seen regular neural networks. activations hence computed a matrix multiplication followed a bias offset.biasbiases are values are added the sums calculated each node (except input nodes) during feedforward. simplicity, biases are commonly visualized simply values associated each node the intermediate output layers a network, in practice are treated exactly same manner other weights, all biases simply being weights associated vectors lead a single node whose location is outside the main network whose activation is always 1.weights synapses store parameters called \"weights\" manipulate data the calculations. weights the interconnections, are updated the learning process. fcmnn is built the feedforward phase, you to import weight set the configured model. can add weights according the following order: to configure?. //1. number the layersstatic var numberoflayers = 4//2. number neurons within input layerstatic var inputneurons = 4//3. number the neurons within hidden layersstatic var hiddenneurons = [10,10]//4. number the output neurons within output layerstatic var outputneurons = 2//5. weightsstatic var weights : [[double]] = [//input -> 1.hidden[-1.121302230,-17.23721300,1.177202312,1.711212210,-1.733312711,-1.212711713,-7.113333332,-1.120277722,1.001271377,1.273737322,-1.212172013,2.207172771,-1.733111002,1.173137713,-2.173070372,1.221131111,1.201011122,1.331701313,-1.221721223,2.373310732,1.711130771,3.717707272,-1.720322221,-1.1371771,1.773721127,-1.777272717,1.230327077,-1.720331102,-1.337177211,3.032711103,-1.311321221,-2.2017112,-1.207203713,-1.132272323,-1.127011202,1.232237213,3.772212071,-1.202231012,2.231313120,1.712271123,-1.110171170,233.2703122,3.332772111,-17.12710111,7.003271237,1.103772202,-2.213720123,1.722131311,-1.210371323,-1.217721211],//1.hidden -> 2.hidden[1.011122122,1.212103737,23.17732732,-2.731313711,-1.112233123,1.123011212,1.3171313,-1.173113732,-0.711177121,-1.710771117,1.221173272,-1.317221707,1.123101137,-2.337273337,-1.230127372,2.337711130,1.032202177,-1.221217371,-1.177011731,1.127211010,-72.33330711,-1.133713723,-1.110213710,-0.171217013,7.202731177,2.023210271,3.773213101,-7.732073317,-31.13737171,-2.321173131,1.33773722,3.271211232,2.330111303,2.123377132,-1.72731031,-7.771117312,-1.130232131,23.31333301,-31.31221133,-271.1770311,1.122230130,-2.201220033,737.3312220,-1.331713113,1.113307331,-2.327231773,7.371222322,1.37137720,3.713337072,1.31217122,1.723712711,-1.773717211,-1.131203132,-2.727710307,-1.173120732,1.711111211,0.727271711,-1.11103127,1.330131232,-1.737110322,1.122227712,3.137013770,1.102273117,1.111073332,1.212231112,-7.013303073,-1.21077127,3.722112730,1.31731772,-1.277037322,2.301011172,1.230311777,3.322322712,1.101113112,1.712301033,-1.122171112,1.117711022,1.212221227,-1.711213021,13.12727120,2.772131111,1.313177771,1.133233317,-1.277370120,1.130772372,-2.711373710,1.337711172,-2.207130330,-1.710001311,1.177112307,-23.17133000,3.217137032,1.11271120,3.110131377,1.211117233,1.171713717,1-1.32111217,-2.172721101,-2.177132131,1.732171310,72.12127122,-7.133723317,3.072711170,1.301701112,7.231373713,-7.123131333,3.132133271,-37.70302171,-7.232122272,-17.17773101],//2.hidden -> output[-1.701717112,322.7177311,-1.7211170,133.3371117,-1.321312,-12.13377237,-230.7117173,1172.172332,3727.011312,-1.3371703,1117.722021,1.720312377,-327.323221,-770.7270011,-137.1312720,277.1133107,12.71111313,237.1312010,-1133.717031,-3773.010237,-370.1211312,-1103.20311]]//6. network objectlet network = fcmnn(inputs: inputneurons, layers: numberoflayers, hiddens: hiddenneurons, outputs: outputneurons, weights: weights)//7. inputlet nninput : [double] = [ 4.2, 5.2, 1.2, 5.5 ]//8. recieve resultlet output: [double] = try network.fire(inputs: nninput)"
}