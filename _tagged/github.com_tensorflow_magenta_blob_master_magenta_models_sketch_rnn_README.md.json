{
	"_id": "14431045",
	"site": "https://github.com/tensorflow/magenta/blob/master/magenta/models/sketch_rnn/README.md",
	"title": "Sketch-RNN: A Generative Model for Vector Drawings",
	"author": "dsr12",
	"date": "2017-06-13T13:55:34.420Z",
	"tags": {
		"categories": [
			"opensource"
		],
		"languages": []
	},
	"content": "sketch-rnn: generative model vector drawingsbefore jumping on code examples, please set your magenta environment.examples vector images produced this generative model. repo contains tensorflow code sketch-rnn, recurrent neural network model described teaching machines draw a neural representation sketch drawings.overview modelsketch-rnn is sequence--sequence variational autoencoder. encoder rnn is bi-directional rnn, the decoder is autoregressive mixture-density rnn. can specify type rnn cell use, the size the rnn using settings enc_model, dec_model, enc_size, dec_size. encoder sample latent code z, vector floats a dimension z_size. in vae, can enforce gaussian iid distribution z, the strength the kl divergence loss term is controlled using kl_weight. will a tradeoff between kl divergence loss the reconstruction loss. also allow room the latent code store information, not pure gaussian iid. once kl loss term gets below kl_tolerance, will stop optimizing this term. small medium sized datasets, dropout data augmentation is very useful technique avoid overfitting. have provided options input dropout, output dropout, recurrent dropout without memory loss. practice, only recurrent dropout, usually set to between 65% 90% depending the dataset. layer normalization recurrent dropout be used together, forming powerful combination training recurrent neural nets a small dataset. are data augmentation techniques provided. first is random_scale_factor randomly scale size training images. second augmentation technique ( used the sketch-rnn paper) is dropping random points a line stroke. given line segment more 2 points, can randomly drop points inside line segments a small probability augment_stroke_prob, still maintain similar-looking vector image. type data augmentation is very powerful used small datasets, is unique vector drawings, since is difficult dropout random characters notes text midi data, also possible dropout random pixels without causing large visual differences pixel image data. usually set both data augmentation parameters 10% 20%. there is virtually difference a human audience they compare augmented example compared a normal example, apply both data augmentation techniques regardless the size the training dataset.using dropout data augmentation effectively avoid overfitting a small training set.training model train model first need dataset containing train/validation/test examples. have provided links the aaron_sheep dataset the model use lightweight dataset default.example usage:sketch_rnn_train --log_root=checkpoint_path --data_dir=dataset_path --hparams=\"data_set=[dataset_filename.npz]\" recommend create subdirectories inside models datasets hold own data checkpoints. tensorboard logs be stored inside checkpoint_path viewing training curves the various losses train/validation/test datasets.here is list full options the model, along the default settings:data_set=['aaron_sheep.npz'], # dataset. be list multiple .npz sets.num_steps=10000000, # total number training set. keep large.save_every=500, # number batches per checkpoint creation.dec_rnn_size=512, # size decoder.dec_model='lstm', # decoder: lstm, layer_norm hyper.enc_rnn_size=256, # size encoder.enc_model='lstm', # encoder: lstm, layer_norm hyper.z_size=128, # size latent vector z. recommend 32, 64 128.kl_weight=0.5, # kl weight loss equation. recommend 0.5 1.0.kl_weight_start=0.01, # kl start weight annealing.kl_tolerance=0.2, # level kl loss which stop optimizing kl.batch_size=100, # minibatch size. recommend leaving 100.grad_clip=1.0, # gradient clipping. recommend leaving 1.0.num_mixture=20, # number mixtures gaussian mixture model.learning_rate=0.001, # learning rate.decay_rate=0.9999, # learning rate decay per minibatch.kl_decay_rate=0.99995, # kl annealing decay rate per minibatch.min_learning_rate=0.00001, # minimum learning rate.use_recurrent_dropout=true, # recurrent dropout without memory loss. recomended.recurrent_dropout_prob=0.90, # probability recurrent dropout keep.use_input_dropout=false, # input dropout. recommend leaving false.input_dropout_prob=0.90, # probability input dropout keep.use_output_dropout=false, # output droput. recommend leaving false.output_dropout_prob=0.90, # probability output dropout keep.random_scale_factor=0.15, # random scaling data augmention proportion.augment_stroke_prob=0.10, # point dropping augmentation proportion.conditional=true, # false, decoder- model.here are options may to to train model a very large dataset spanning three .npz files, use hyperlstm the rnn cells. small datasets less 10k training examples, lstm layer normalization (layer_norm both enc_model dec_model) works best.sketch_rnn_train --log_root=models/big_model --data_dir=datasets/big_dataset --hparams=\"data_set=[class1.npz,class2.npz,class3.npz],dec_model=hyper,dec_rnn_size=2048,enc_model=layer_norm,enc_rnn_size=512,save_every=5000,grad_clip=1.0,use_recurrent_dropout=0\" have tested model tensorflow 1.0 1.1 python 2.7.datasetsdue size limitations, repo does contain datasets. have prepared many datasets work of box sketch-rnn. google quickdraw dataset is collection 50m vector sketches across 345 categories. the repo quickdraw-dataset, is section called sketch-rnn quickdraw dataset describes pre-processed datafiles can used this project. each category class is stored its own file, such cat.npz, contains training/validation/test set sizes 70000/2500/2500 examples. download .npz datasets google cloud local . we recommend create sub directory called datasets/quickdraw, save .npz files this sub directory. addition the quickdraw dataset, have tested model smaller datasets. the sketch-rnn-datasets repo, are 3 datasets: aaron koblin sheep market, kanji, omniglot. recommend create sub directory each these dataset, such datasets/aaron_sheep, you wish use locally. mentioned before, recurrent dropout data augmentation should used training models small datasets avoid overfitting.creating own datasetplease create own interesting datasets train algorithm them! getting hands dirty creating datasets is part the fun. why settle existing pre-packaged datasets you are potentially sitting an interesting dataset vector line drawings? our experiments, dataset size consisting a few thousand examples was sufficient produce meaningful results. here, describe format the dataset files model expects see.each example the dataset is stored list coordinate offsets: x, y, a binary value representing whether pen is lifted away the paper. format, refer as stroke-3, is described this paper. note the data format described the paper has 5 elements (stroke-5 format), this conversion is done automatically inside dataloader. below is example sketch a turtle using format:figure: sample sketch, a sequence (x, y, binary pen state) points in rendered form. the rendered sketch, line color corresponds the sequential stroke ordering. our datasets, each example the list examples is represented a np.array np.int16 datatypes. can store as np.int8 you get away it save storage space. your data must in floating-point format, np.float16 works. np.float32 be waste storage space. our data, x y offsets are represented pixel locations, are larger the range numbers neural network model likes see, there is normalization scaling process built the model. we load training data, model automatically convert np.float normalize accordingly before training. you to create own dataset, must create three lists examples training/validation/test sets, avoid overfitting the training set. model handle early stopping using validation set. the aaron_sheep dataset, used split 7400/300/300 examples, put each inside python lists called train_data, validation_data, test_data. afterwards, created subdirectory called datasets/aaron_sheep we the built- savez_compressed method save compressed version the dataset a aaron_sheep.npz file. all our experiments, size each dataset is exact multiple 100, use batch_size 100. deviate your own peril.filename = os.path.join('datasets/your_dataset_directory', 'your_dataset_name.npz')np.savez_compressed(filename, train=train_data, valid=validation_data, test=test_data) also performed simple stroke simplification preprocess data, called ramer-douglas-peucker. is easy--use open source code applying algorithm here. practice, can set epsilon parameter a value between 0.2 3.0, depending how aggressively want simply lines. the paper used epsilon parameter 2.0. suggest build dataset where maximum sequence length is less 250. you a large set simple svg images, are available libraries convert subsets svgs line segments, you then apply rdp the line segments before converting data stroke-3 format.pre-trained models have provided pre-trained models the aaron_sheep dataset, both conditional unconditional training mode, using vanilla lstm cells lstm cells layer normalization. models be downloaded running jupyter notebook. are stored :/tmp/sketch_rnn/models/aaron_sheep/lstm/tmp/sketch_rnn/models/aaron_sheep/lstm_uncond/tmp/sketch_rnn/models/aaron_sheep/layer_norm/tmp/sketch_rnn/models/aaron_sheep/layer_norm_uncond addition, have provided pre-trained models selected quickdraw datasets:/tmp/sketch_rnn/models/owl/lstm/tmp/sketch_rnn/models/flamingo/lstm_uncond/tmp/sketch_rnn/models/catbus/lstm/tmp/sketch_rnn/models/elephantpig/lstmusing model jupyter notebooklet's the model interpolate between cat a bus!'ve included simple jupyter notebook show how load pre-trained model generate vector sketches. will able encode, decode, morph between vector images, also generate random ones. sampling images, can tune temperature parameter control level uncertainty.citation you find project useful academic purposes, please cite as:@article{sketchrnn, author = {{ha}, david {eck}, douglas}, title = \"{ neural representation sketch drawings}\", journal = {arxiv e-prints}, archiveprefix = \"arxiv\", eprinttype = {arxiv}, eprint = {1704.03477}, primaryclass = \"cs.ne\", keywords = {computer science - neural evolutionary computing, computer science - learning, statistics - machine learning}, = 2017, month = apr,}"
}