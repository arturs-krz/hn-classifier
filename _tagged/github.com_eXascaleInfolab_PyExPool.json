{
	"_id": "14444965",
	"site": "https://github.com/eXascaleInfolab/PyExPool",
	"title": " PyExPool â€“ Tiny Multi-Process Execution Pool with Adjustments for NUMA",
	"author": "luav",
	"date": "2017-06-13T13:06:17.956Z",
	"tags": {
		"categories": [
			"opensource",
			"multiprocessing",
			"parallel-computing",
			"execution-pool",
			"cache-control",
			"numa"
		],
		"languages": [
			"python"
		]
	},
	"content": "readme.md pyexpool lightweight multi-process execution pool schedule jobs execution per-job timeout, optionally grouping into tasks specifying optional execution parameters considering numa architecture:automatic cpu affinity management maximization the dedicated cpu cache a worker processminimal amount ram per worker processtimeout per each job ( was main motivation implement module, this feature is provided any python implementation of box)onstart/ondone callbacks, ondone is called on successful completion ( termination) both jobs tasks (group jobs)stdout/err output, can redirected any custom file pipecustom parameters each job embracing task besides name/idimplemented a single-file module be easily included your project customized a part your distribution ( in pycabem), as separate library. main purpose this single-file module is asynchronous execution modules external executables. case asynchronous execution the python functions is required usage external dependences is a problem, more handy straightforward approach is use pebble library.\\author: (c) artem lutov artem@exascale.info\\organizations: exascale infolab, lumais, sciencewise\\date: 2016-01contentapijobtaskexecpoolusageusage examplefailsafe terminationrelated projectsapiflexible api provides automatic cpu affinity management, maximization the dedicated cpu cache limitation the minimal dedicated ram per worker process, optional automatic restart jobs timeout, access job's process, parent task, start stop execution and more...execpool represents pool worker processes execute jobs can grouped taskss more flexible management.jobjob(name, workdir=none, args=(), timeout=0, ontimeout=false, task=none, startdelay=0, onstart=none, ondone=none, params=none, stdout=sys.stdout, stderr=sys.stderr, omitafn=false):\"\"\"initialize job be executedname - job nameworkdir - working directory the corresponding process, none means dir the benchmarkingargs - execution arguments including executable itself the processnote: be none make a stub process execute callbackstimeout - execution timeout. default: 0, means infinityontimeout - action timeout:false - terminate job. defaulttrue - restart jobtask - origin task this job is part the taskstartdelay - delay the job process starting execute for time,executed the context the caller (main process).attention: should small (0.1 .. 1 sec)onstart - callback is executed the job starting (before executionstarted) the context the caller (main process) the single argument, job. default: noneattention: must lightweightnote: be executed few times the job is restarted timeoutondone - callback is executed successful completion the job thecontext the caller (main process) the single argument, job. default: noneattention: must lightweightparams - additional parameters be used callbacksstdout - none file name pipe the buffered output be appendedstderr - none file name pipe stdout the unbuffered error output be appendedattention: pipe is buffer ram, do use if output data is huge unlimitedomitafn - omit affinity policy the scheduler, is actual the affinity is enabled the process has multiple treadststart - start is filled automatically the execution start (before onstart). default: nonetstop - termination / completion after ondoneproc - process the job, be used the ondone() read 's pipe\"\"\"tasktask(name, timeout=0, onstart=none, ondone=none, params=none, stdout=sys.stdout, stderr=sys.stderr):\"\"\"initialize task, is group jobs be executedname - task nametimeout - execution timeout. default: 0, means infinityonstart - callback is executed the task starting (before executionstarted) the context the caller (main process) the single argument, task. default: noneattention: must lightweightondone - callback is executed successful completion the task thecontext the caller (main process) the single argument, task. default: noneattention: must lightweightparams - additional parameters be used callbacksstdout - none file name pipe the buffered output be appendedstderr - none file name pipe stdout the unbuffered error output be appendedattention: pipe is buffer ram, do use if output data is huge unlimitedtstart - start is filled automatically the execution start (before onstart). default: nonetstop - termination / completion after ondone\"\"\"execpooldef ramfracs(fracsize):\"\"\"evaluate minimal number ram fractions the specified size gbused estimate reasonable number processes the specified minimaldedicated ram.fracsize - minimal size each fraction gb, be fractional numberreturn minimal number ram fractions having specified size gb\"\"\"def cpucorethreads():\"\"\" number hardware treads per cpu coreused specify cpu afinity step dedicating maximal amount cpu cache.\"\"\"def cpunodes():\"\"\" number numa nodes, where cpus are locatedused evaluate cpu index the affinity table index considerin numa architectore.\"\"\"def afnicpu(iafn, corethreads=1, nodes=1, crossnodes=true):\"\"\"affinity table index mapping the cpu indexaffinity table is reduced cpu table the non-primary hw treads each core.typically cpus are evumerated across nodes:numa node0 cpu(s): 0,2,4,6,8,10,12,14,16,18,20,22,24,26,28,30numa node1 cpu(s): 1,3,5,7,9,11,13,15,17,19,21,23,25,27,29,31, in case number threads per core is 2 the following cpus should bound:0, 1, 4, 5, 8, ...2 -> 4, 4 -> 8# -> i + // cpunodes() * cpunodes() * (cpucorethreads() - 1)iafn - index the affinity table be mapped the respective cpu indexcorethreads - hw threads per cpu core just affinity step,1 - maximal parallelization the minimal cpu cache sizenodes - numa nodes containing cpuscrossnodes - cross-nodes enumeration the cpus the numa nodesreturn cpu index respective the specified index the affinity table\"\"\"execpool(workers=cpu_count(), afnstep=none)\"\"\"multi-process execution pool jobsworkers - number resident worker processes, >=1. reasonable value is<= numa nodes * node cpus, is typically returned cpu_count(),where node cpus = cpu cores * hw treads per core. guarantee minimal number ram per process, example 2.5 gb:workers = min(cpu_count(), max(ramfracs(2.5), 1))afnstep - affinity step, integer applied. used bound whole cpu coresinstead the hardware treads have more dedicated cache.typical values:none - not affinity all (recommended multi-threaded workers),1 - maximize parallelization ( number worker processes = cpu units),cpucorethreads() - maximize dedicated cpu cache ( number worker processes = cpu cores = cpu units / hardware treads per cpu core).note: specification the afnstep might cause reduction the workers.\"\"\"execute(job, async=true):\"\"\"schedule job the executionjob - job be executed, instance jobasync - async execution wait until execution completed note: sync tasks are started oncereturn - 0 successful execution, process return code otherwise\"\"\"join(timeout=0):\"\"\"execution cycletimeout - execution timeout seconds before workers termination, >= 0.0 means absence the timeout. time is measured since first jobwas scheduled until completion all scheduled jobs.return - true graceful completion, false termination the specified timeout\"\"\"__del__():\"\"\"force termination the pool\"\"\"__finalize__():\"\"\"force termination the pool\"\"\"usagetarget version the python is 2.7+ including 3.x, works fine pypy. workflow consists the following steps:create execution pool.create schedule jobs required parameters, callbacks optionally packing into tasks.wait execution pool until the jobs are completed terminated, until global timeout is elapsed.usage example multiprocessing import cpu_count sys import executable pyexec # full path the current python interpreter# 1. create multi-process execution pool the optimal affinity step maximize dedicated cpu cache sizeexecpool = execpool(max(cpu_count() - 1, 1), cpucorethreads())global_timeout = 30 * 60 # 30 min, timeout execute scheduled jobs terminate # 2. schedule jobs execution the pool# 2. job scheduling using external executable: \"ls -la\"execpool.execute(job(name='list_dir', args=('ls', '-la')))# 2.b job scheduling using python function / code fragment,# is a goal the design, is possible.# 2.b.1 create job specified parametersjobname = 'netshuffling'jobtimeout = 3 * 60 # 3 min# network shuffling routine be scheduled a job,# can be call any external executable ( 2.alt below)args = (pyexec, '-c',\"\"\"import osimport subprocessbasenet = '{jobname}' + '{_extnetfile}'#print('basenet: ' + basenet, file=sys.stderr) i range(1, {shufnum} + 1):netfile = ''.join(('{jobname}', '.', str(), '{_extnetfile}')) {overwrite} not os.path.exists(netfile):# sort -r pgp_udir.net -o pgp_udir_rand3.netsubprocess.call(('sort', '-r', basenet, '-o', netfile))\"\"\".format(jobname=jobname, _extnetfile='.net', shufnum=5, overwrite=false))# 2.b.2 schedule job execution, might postponed# there are any free executor processes availableexecpool.execute(job(name=jobname, workdir='this_sub_dir', args=args, timeout=jobtimeout# note: onstart/ondone callbacks, custom parameters others be specified here!))# add another jobs# ...# 3. wait the jobs execution the specified timeout mostexecpool.join(global_timeout) # 30 minfailsafe termination perform graceful termination the jobs case external termination your program, signal handlers be set:import signal # intercept kill signals# execpool a global variable, is set none all jobs are done,# recreated jobs schedulingexecpool = nonedef terminationhandler(signal, frame):\"\"\"signal termination handler\"\"\"global execpool execpool:del execpool # destructors are called latersys.exit(0)# set handlers external signals, can the lines inside# __name__ == '__main__':signal.signal(signal.sigterm, terminationhandler)signal.signal(signal.sighup, terminationhandler)signal.signal(signal.sigint, terminationhandler)signal.signal(signal.sigquit, terminationhandler)signal.signal(signal.sigabrt, terminationhandler)# define execpool schedule jobsexecpool = execpool(max(cpu_count() - 1, 1))# failsafe usage execpool ... it is recommended register termination handler the normal interpreter termination using atexit:import atexit...atexit.register(terminationhandler)note: please, star project you it.related projectsexectime - failover lightweight resource consumption profiler (timings memory), applicable multiple processes optional per-process results labeling synchronized output the specified file stderr: https://bitbucket.org/lumais/exectime"
}