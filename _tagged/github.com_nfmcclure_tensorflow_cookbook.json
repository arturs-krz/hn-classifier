{
	"_id": "14459937",
	"site": "https://github.com/nfmcclure/tensorflow_cookbook",
	"title": "Code for Tensorflow Machine Learning Cookbook",
	"author": "ibobriakov",
	"date": "2017-06-13T13:55:02.140Z",
	"tags": {
		"categories": [
			"opensource",
			"tensorflow",
			"tensorflow-cookbook",
			"linear-regression",
			"neural-network",
			"tensorflow-algorithms",
			"rnn",
			"cnn",
			"svm",
			"nlp",
			"packtpub",
			"machine-learning",
			"tensorboard",
			"classification",
			"regression",
			"kmeans-clustering",
			"genetic-algorithm",
			"ode"
		],
		"languages": [
			"jupyter notebook",
			"python"
		]
	},
	"content": "readme.md tensorflow machine learning cookbook(code is slowly becoming tensorflow-v1.0.1 compliant.) packt publishing book nick mcclure=================build: =================table contentsch 1: getting started tensorflowch 2: tensorflow ch 3: linear regressionch 4: support vector machinesch 5: nearest neighbor methodsch 6: neural networksch 7: natural language processingch 8: convolutional neural networksch 9: recurrent neural networksch 10: taking tensorflow productionch 11: more tensorflowch 1: getting started tensorflow chapter intends introduce main objects concepts tensorflow. also introduce to access data the rest the book provide additional resources learning tensorflow.general outline tf algorithmshere introduce tensorflow the general outline how tensorflow algorithms .creating using tensors to create initialize tensors tensorflow. also depict these operations appear tensorboard.using variables placeholders to create use variables placeholders tensorflow. also depict these operations appear tensorboard.working matricesunderstanding tensorflow work matrices is crucial understanding the algorithms .declaring operations to various mathematical operations tensorflow.implementing activation functionsactivation functions are unique functions tensorflow has built for use algorithms.working data sourceshere show to access the various required data sources the book. are links describing data sources where come .additional resourcesmostly official resources papers. papers are tensorflow papers deep learning resources.ch 2: tensorflow after have established basic objects methods tensorflow, now to establish components make tensorflow algorithms. start introducing computational graphs, then move loss functions back propagation. end creating simple classifier then show example evaluating regression classification algorithms. operation a computational graph show to create operation a computational graph how visualize using tensorboard.layering nested operations show to create multiple operations a computational graph how visualize using tensorboard.working multiple layershere extend usage the computational graph create multiple layers show they appear tensorboard.implementing loss functions order train model, must able evaluate well is doing. is given loss functions. plot various loss functions talk the benefits limitations some.implementing propagationhere show to loss functions iterate through data back propagate errors regression classification.working stochastic batch trainingtensorflow makes easy use both batch stochastic training. show to implement both talk the benefits limitations each.combining everything together now combine everything together we learned create simple classifier.evaluating models model is as as 's evaluation. here show examples (1) evaluating regression algorithm (2) classification algorithm.ch 3: linear regression here show to implement various linear regression techniques tensorflow. first sections show to standard matrix linear regression solving tensorflow. remaining six sections depict to implement various types regression using computational graphs tensorflow.using matrix inverse method to solve 2d regression a matrix inverse tensorflow.implementing decomposition methodsolving 2d linear regression cholesky decomposition.learning tensorflow of linear regressionlinear regression iterating through computational graph l2 loss.understanding loss functions linear regressionl2 vs l1 loss linear regression. talk the benefits limitations both.implementing deming regression (total regression)deming (total) regression implemented tensorflow changing loss function.implementing lasso ridge regressionlasso ridge regression are ways regularizing coefficients. implement both these tensorflow via changing loss functions.implementing elastic net regressionelastic net is regularization technique combines l2 l1 loss coefficients. show to implement in tensorflow.implementing logistic regression implement logistic regression the of activation function our computational graph.ch 4: support vector machines chapter shows to implement various svm methods tensorflow. first create linear svm also show it be used regression. then introduce kernels (rbf gaussian kernel) show to it split non-linear data. finish a multi-dimensional implementation non-linear svms work multiple classes.introduction introduce concept svms how will about implementing in tensorflow framework.working linear svms create linear svm separate . setosa based sepal length pedal width the iris data set.reduction linear regression heart svms is separating classes a line. change tweek algorithm slightly perform svm regression.working kernels tensorflow order extend svms non-linear data, explain show to implement different kernels tensorflow.implementing non-linear svms use gaussian kernel (rbf) separate non-linear classes.implementing multi-class svmssvms are inherently binary predictors. show to extend in one-vs- strategy tensorflow.ch 5: nearest neighbor methods nearest neighbor methods are very popular ml algorithm. show to implement k-nearest neighbors, weighted k-nearest neighbors, k-nearest neighbors mixed distance functions. this chapter also show to the levenshtein distance (edit distance) tensorflow, use to calculate distance between strings. end chapter showing to k-nearest neighbors categorical prediction the mnist handwritten digit recognition.introduction introduce concepts methods needed performing k-nearest neighbors tensorflow.working nearest neighbors create nearest neighbor algorithm tries predict housing worth (regression).working text based distances order use distance function text, show to edit distances tensorflow.computing mixing distance functionshere implement scaling the distance function the standard deviation the input feature k-nearest neighbors.using address matching use mixed distance function match addresses. use numerical distance zip codes, string edit distance street names. street names are allowed have typos.using nearest neighbors image recognition mnist digit image collection is great data set illustration how perform k-nearest neighbors an image classification task.ch 6: neural networks neural networks are very important machine learning growing popularity due the major breakthroughs prior unsolved problems. must start introducing 'shallow' neural networks, are very powerful can help improve prior ml algorithm results. start introducing very basic nn unit, operational gate. gradually add more more the neural network end training model play tic-tac-toe.introduction introduce concept neural networks how tensorflow is built easily handle algorithms.implementing operational gates implement operational gate one operation. we show to extend to multiple nested operations.working gates activation functions we to introduce activation functions the gates. show different activation functions operate.implementing one layer neural network have the pieces start implementing first neural network. do here regression the iris data set.implementing different layers section introduces convolution layer the max-pool layer. show to chain together a 1d 2d example fully connected layers well.using multi-layer neural networkshere show to functionalize different layers variables a cleaner multi-layer neural network.improving predictions linear models show we improve convergence our prior logistic regression a set hidden layers.learning play tic-tac-toegiven set tic-tac-toe boards corresponding optimal moves, train neural network classification model play. the end the script, can attempt play against trained model.ch 7: natural language processing natural language processing (nlp) is way processing textual information numerical summaries, features, models. this chapter will motivate explain to best deal text tensorflow. show to implement classic 'bag--words' show there may better ways embed text based the problem hand. are neural network embeddings called word2vec (cbow skip-gram) doc2vec. show to implement of in tensorflow.introduction introduce methods turning text numerical vectors. introduce tensorflow 'embedding' feature well.working bag--wordshere use tensorflow do one-hot-encoding words called bag--words. use method logistic regression predict a text message is spam ham.implementing tf-idf implement text frequency - inverse document frequency (tfidf) a combination sci-kit learn tensorflow. perform logistic regression tfidf vectors improve our spam/ham text-message predictions.working skip-gram first implementation word2vec called, \"skip-gram\" a movie review database.working cbownext, implement form word2vec called, \"cbow\" (continuous bag words) a movie review database. also introduce method saving loading word embeddings.implementing word2vec example this example, use prior saved cbow word embeddings improve our tf-idf logistic regression movie review sentiment.performing sentiment analysis doc2vechere, introduce doc2vec method (concatenation doc word embeddings) improve logistic model movie review sentiment.ch 8: convolutional neural networks convolutional neural networks (cnns) are ways getting neural networks deal image data. cnn derive name the of convolutional layer applies fixed size filter across larger image, recognizing pattern any part the image. are many tools they (max-pooling, dropout, etc...) we show to implement tensorflow. also show to retrain existing architecture take cnns further stylenet deep dream.introduction introduce convolutional neural networks (cnn), how can them tensorflow.implementing simple cnn.here, show to create cnn architecture performs on mnist digit recognition task.implementing advanced cnn. this example, show to replicate architecture the cifar-10 image recognition task.retraining existing architecture. show to download setup cifar-10 data the tensorflow retraining/fine-tuning tutorial.using stylenet/neuralstyle. this recipe, show basic implementation using stylenet neuralstyle.implementing deep dream. script shows line--line explanation tensorflow's deepdream tutorial. taken deepdream tensorflow. note the code here is converted python 3.ch 9: recurrent neural networks recurrent neural networks (rnns) are very similar regular neural networks except they allow 'recurrent' connections, loops depend the prior states the network. allows rnns efficiently deal sequential data, whereas types networks cannot. then motivate usage lstm (long short term memory) networks a of addressing regular rnn problems. we show easy is implement rnn types tensorflow.introduction introduce recurrent neural networks how are able feed a sequence predict either fixed target (categorical/numerical) another sequence (sequence sequence).implementing rnn model spam prediction this example, create rnn model improve our spam/ham sms text predictions.implementing lstm model text generation show to implement lstm (long short term memory) rnn shakespeare language generation. (word level vocabulary)stacking multiple lstm layers stack multiple lstm layers improve our shakespeare language generation. (character level vocabulary)creating sequence sequence translation model (seq2seq)here, use tensorflow's sequence--sequence models train english-german translation model.training siamese similarity measurehere, implement siamese rnn predict similarity addresses use for record matching. using rnns record matching is very versatile, we not a fixed set target categories can the trained model predict similarities across addresses.ch 10: taking tensorflow production course is more tensorflow just creating fitting machine learning models. once have model we to , we to move towards production usage. chapter provide tips examples implementing unit tests, using multiple processors, using multiple machines (tensorflow distributed), finish a full production example.implementing unit tests show to implement different types unit tests tensors (placeholders variables).using multiple executors (devices) to a machine multiple devices. e.g., machine a cpu, one more gpus.parallelizing tensorflow to setup use tensorflow distributed multiple machines.tips tensorflow productionvarious tips developing tensorflow example productionalizing tensorflow show to take rnn model predicting ham/spam ( chapter 9, recipe #2) put in production level files: training evaluation.ch 11: more tensorflow illustrate versatile tensorflow is, will show additional examples this chapter. start showing to the logging/visualizing tool tensorboard. we illustrate to k-means clustering, a genetic algorithm, solve system odes.visualizing computational graphs ( tensorboard) example using histograms, scalar summaries, creating images tensorboard.working a genetic algorithm create genetic algorithm optimize individual (array 50 numbers) toward ground truth function.clustering using k-means to tensorflow do k-means clustering. use iris data set, set k=3, use k-means make predictions.solving system odeshere, show to tensorflow solve system odes. system concern is lotka-volterra predator-prey system."
}