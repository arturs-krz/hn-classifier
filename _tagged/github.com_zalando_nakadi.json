{
	"_id": "14443123",
	"site": "https://github.com/zalando/nakadi",
	"title": "Nakadi: Distributed event bus that implements a RESTful API abstraction",
	"author": "vishesh92",
	"date": "2017-06-13T13:55:21.858Z",
	"tags": {
		"categories": [
			"opensource",
			"java",
			"apis",
			"event-bus",
			"restful",
			"microservices",
			"postgresql",
			"kafka",
			"java-8"
		],
		"languages": [
			"java",
			"other"
		]
	},
	"content": "readme.md table contentsnakadi event brokerquickstartrunning serverstopping servermac os docker settingsapi overview usageevents event typescreating event typescreate event typelist event typesview event typelist partitions an event typeview partition an event typepublishing eventsposting or more eventsconsuming eventsopening event streamevent stream structurecursors, offsets partitionsevent stream keepalivessubscriptionscreating subscriptionsconsuming events a subscriptionclient rebalancingsubscription cursorscommitting cursorschecking current positionsubscription statisticsdeleting subscriptiongetting listing subscriptionsbuild developmentbuildingdependencies does project already implement?contributingnakadi event broker goal nakadi ( means \"stream\" georgian) is provide event broker infrastructure :abstract event delivery via secured restful api. allows microservices teams maintain service boundaries, not directly depend any specific message broker technology. access the api be managed secured using oauth scopes.enable convenient development event-driven applications asynchronous microservices. event types be defined schemas managed via registry. nakadi has optional support events describing business processes data changes using standard primitives identity, timestamps, event types, causality.efficient low latency event delivery. once publisher sends event using simple http post, consumers be pushed via streaming http connection, allowing near real- event processing. consumer connection has keepalive controls support managing stream offsets. project provides compatability the stups project. additional features we plan cover the future are:discoverability the resource structures flowing the broker. managed api allows consumers subscribe have stream offsets stored the server.filtering events subscribing consumers.role based access control data.support different streaming technologies engines. nakadi currently uses apache kafka its broker, other providers (such kinesis) be possible.more detailed information be found the manual.quickstart can run project locally using docker. note nakadi requires very recent versions docker docker-compose. dependencies more information.running server the project's home directory can start nakadi via gradle:./gradlew startnakadi will build project run docker compose 4 services:nakadi (8080)postgresql (5432)kafka (9092)zookeeper (2181)stopping server stop running nakadi:./gradlew stopnakadimac os docker settingssince docker mac os runs inside virtual box, will to expose ports to allow nakadi access dependencies:docker-machine ssh default \\-l 9092:localhost:9092 \\-l 8080:localhost:8080 \\-l 5432:localhost:5432 \\-l 2181:localhost:2181alternatively can set port forwarding the \"default\" machine through network settings the virtualbox ui. you the message \"is docker daemon running this host?\" you docker virtualbox arerunning, might to run command:eval \"$(docker-machine env default)\"note: docker mac os (previously beta) version 1.12 (1.12.0 1.12.1) currently is supported due the bug networking host configuration.api overview usageevents event types nakadi api allows publishing consuming events http. do the producer must register event type the nakadi schemaregistry. event type contains information such the name, owning application,strategies partitioning enriching data, a json schema. once event type is created, publishing resource becomes available will acceptevents the type, consumers also read the event stream. are three main categories event type defined nakadi -undefined: free form category suitable events are entirely custom the producer.data: event represents change a record other item, a item. change events are associated a create, update, delete, snapshot operation.business: event is part , or drives business process, such a state transition a customer order. events the business data change helper categories follow generic nakadi event schema well a schema custom the event data. genericschema pre-defines common fields an event the custom schema the eventis defined the event type is created. a json event one thesecategories is posted the server, is expected conform thecombination the generic schema the category to custom schema defined the event type. combination is called effective schema isvalidated nakadi. undefined category is required have json schema creation, this be simple { \"\\additionalproperties\\\": true } allow arbitraryjson. unlike business data categories, schema an undefined type is checked nakadi an event is posted, it be used a consumer validate data the stream.creating event typescreate event type event type be created posting the event-types resource.each event type must a unique name. the event type already exists 409 conflict response be returned. otherwise successful request result a 201 created response. exact required fields depend theevent type's category, name, owning_application schema are alwaysexpected. schema value should declare custom part the event - genericschema is implicit doesn't need be defined. combination the (the \"effective schema\") be checked events are submitted the event type.each event type have default_statistic object attached. controls number partitions the underlying topic. you not provide value,nakadi use sensible default value may just single partition. will effectively disallow parallel reads subscriptions this eventtype. values provided here not changed later, choose wisely. example shows business category event type a simple schema anorder number -curl -v -xpost http://localhost:8080/event-types -h \"content-type: application/json\" -d '{ \"name\": \"order.order_received\", \"owning_application\": \"order-service\", \"category\": \"business\", \"partition_strategy\": \"hash\", \"partition_key_fields\": [\"order_number\"], \"enrichment_strategies\": [\"metadata_enrichment\"], \"default_statistic\": { \"messages_per_minute\": 1000, \"message_size\":5, \"read_parallelism\":1, \"write_parallelism\": 1 }, \"schema\": { \"type\": \"json_schema\", \"schema\": \"{ \\\"properties\\\": { \\\"order_number\\\": { \\\"type\\\": \\\"string\\\" } } }\" }}' example shows undefined category event type a wilcard schema -curl -v -xpost http://localhost:8080/event-types -h \"content-type: application/json\" -d '{ \"name\": \"undef\", \"owning_application\": \"jinteki\", \"category\": \"undefined\", \"partition_strategy\": \"random\", \"schema\": { \"type\": \"json_schema\", \"schema\": \"{ \\\"additionalproperties\\\": true }\" }}' undefined event does accept value enrichment_strategies.list event typescurl -v http://localhost:8080/event-typeshttp/1.1 200 okcontent-type: application/json;charset=utf-8[ { \"category\": \"business\", \"default_statistic\": null, \"enrichment_strategies\": [\"metadata_enrichment\"], \"name\": \"order.order_received\", \"owning_application\": \"order-service\", \"partition_key_fields\": [\"order_number\"], \"partition_strategy\": \"hash\", \"schema\": { \"schema\": \"{ \\\"properties\\\": { \\\"order_number\\\": { \\\"type\\\": \\\"string\\\" } } }\", \"type\": \"json_schema\" } }]view event typeeach event type registered nakadi has uri based its name -curl -v http://localhost:8080/event-types/order.order_receivedhttp/1.1 200 okcontent-type: application/json;charset=utf-8{ \"category\": \"business\", \"default_statistic\": null, \"enrichment_strategies\": [\"metadata_enrichment\"], \"name\": \"order.order_received\", \"owning_application\": \"order-service\", \"partition_key_fields\": [\"order_number\"], \"partition_strategy\": \"hash\", \"schema\": { \"schema\": \"{ \\\"properties\\\": { \\\"order_number\\\": { \\\"type\\\": \\\"string\\\" } } }\", \"type\": \"json_schema\" }}list partitions an event type partitions an event type are available via /partitions resource:curl -v http://localhost:8080/event-types/order.order_received/partitions http/1.1 200 okcontent-type: application/json;charset=utf-8[ { \"newest_available_offset\": \"begin\", \"oldest_available_offset\": \"0\", \"partition\": \"0\" }]view partition an event typeeach partition an event type has uri based its partition value:curl -v http://localhost:8080/event-types/order.order_received/partitions/0 http/1.1 200 okcontent-type: application/json;charset=utf-8{ \"newest_available_offset\": \"begin\", \"oldest_available_offset\": \"0\", \"partition\": \"0\"}publishing eventsposting or more eventsevents an event type be published posting its \"events\" collection:curl -v -xpost http://localhost:8080/event-types/order.order_received/events -h \"content-type: application/json\" -d '[ { \"order_number\": \"24873243241\", \"metadata\": { \"eid\": \"d765de34-09c0-4bbb-8b1e-7160a33a0791\", \"occurred_at\": \"2016-03-15t23:47:15+01:00\" } }, { \"order_number\": \"24873243242\", \"metadata\": { \"eid\": \"a7671c51-49d1-48e6-bb03-b50dcf14f3d3\", \"occurred_at\": \"2016-03-15t23:47:16+01:00\" } }]'http/1.1 200 ok events collection accepts array events. well the fields defined the event type's schema, posted event must contain metadataobject an eid occurred_at fields. eid is uuid uniquelyidentifies event the occurred_at field identifies time creation the event defined the producer.note the order events the posted array be order are publishedonto event stream seen consumers. are re-ordered based their occurred_at other data values.consuming eventsopening event stream can open stream an event type via events sub-resource:curl -v http://localhost:8080/event-types/order.order_received/events event stream structure stream response groups events batches. batches the responseare separated a newline each batch be emitted a singleline, a pretty-printed batch object looks this -{ \"cursor\": { \"partition\": \"0\", \"offset\": \"4\" }, \"events\": [{ \"order_number\": \"24873243241\", \"metadata\": { \"eid\": \"d765de34-09c0-4bbb-8b1e-7160a33a0791\", \"occurred_at\": \"2016-03-15t23:47:15+01:00\" } }, { \"order_number\": \"24873243242\", \"metadata\": { \"eid\": \"a7671c51-49d1-48e6-bb03-b50dcf14f3d3\", \"occurred_at\": \"2016-03-15t23:47:16+01:00\" } }]} cursor object describes partition the offset this batch events. cursor allow clients checkpoint events already beenconsumed navigate through stream - individual events the stream don't cursors. events array contains list events were published the order were posted the producer. each event contain metadatafield well the custom data defined the event type's schema. http response will something this -curl -v http://localhost:8080/event-types/order.order_received/events http/1.1 200 ok{\"cursor\":{\"partition\":\"0\",\"offset\":\"4\"},\"events\":[{\"order_number\": \"order_001\", \"metadata\": {\"eid\": \"4ae5011e-eb01-11e5-8b4a-1c6f65464fc6\", \"occurred_at\": \"2016-03-15t23:56:11+01:00\"}}]}{\"cursor\":{\"partition\":\"0\",\"offset\":\"5\"},\"events\":[{\"order_number\": \"order_002\", \"metadata\": {\"eid\": \"4bea74a4-eb01-11e5-9efa-1c6f65464fc6\", \"occurred_at\": \"2016-03-15t23:57:15+01:00\"}}]}{\"cursor\":{\"partition\":\"0\",\"offset\":\"6\"},\"events\":[{\"order_number\": \"order_003\", \"metadata\": {\"eid\": \"4cc6d2f0-eb01-11e5-b606-1c6f65464fc6\", \"occurred_at\": \"2016-03-15t23:58:15+01:00\"}}]}cursors, offsets partitions default events resource consume all partitions an eventtype from end ( \"tail\") the stream. select particularpartitions a position where the stream start, can supply x-nakadi-cursors header the request:curl -v http://localhost:8080/event-types/order.order_received/events \\ -h 'x-nakadi-cursors: [{\"partition\": \"0\", \"offset\":\"12\"}]' header value is json array cursors. each cursor the arraydescribes partition the stream an offset stream . note events within same partition maintain overall order. offset value the cursor allows select where in stream want consume . this be known offset value, the dedicated valuebegin will start stream the beginning. example, read partition 0 the beginning:curl -v http://localhost:8080/event-types/order.order_received/events \\ -h 'x-nakadi-cursors:[{\"partition\": \"0\", \"offset\":\"begin\"}]' details the partitions their offsets an event type areavailable via partitions resource.event stream keepalives there are events be delivered nakadi keep streaming connection open periodically sending batch no events which contains cursor pointing the current offset. example:curl -v http://localhost:8080/event-types/order.order_received/events http/1.1 200 ok{\"cursor\":{\"partition\":\"0\",\"offset\":\"6\"},\"events\":[{\"order_number\": \"order_003\", \"metadata\": {\"eid\": \"4cc6d2f0-eb01-11e5-b606-1c6f65464fc6\", \"occurred_at\": \"2016-03-15t23:58:15+01:00\"}}]}{\"cursor\":{\"partition\":\"0\",\"offset\":\"6\"}}{\"cursor\":{\"partition\":\"0\",\"offset\":\"6\"}}{\"cursor\":{\"partition\":\"0\",\"offset\":\"6\"}}{\"cursor\":{\"partition\":\"0\",\"offset\":\"6\"}} can treated a keep-alive control some load balancers.subscriptionssubscriptions allow clients consume events, where nakadi server store offsets automatically manages reblancing partitions across consumer clients. allows clients avoid managing stream state locally. typical workflow using subscriptions is:create subscription specifying event-types want read.start reading batches events the subscription.commit cursors found the event batches to nakadi, will store offsets. the connection is closed, later restarted, clients get events the point your last cursor commit. you need more one client yoursubscription distribute load can read subscription multipleclients nakadi balance load across .the following sections provide more detail the subscription api basicexamples subscription api creation usage:creating subscriptions: to create new subscription select event types.consuming events a subscription: to connect and consume batches a susbcription stream.client rebalancing: describes clients a subscription are automatically assigned partitions, how api's -least-once delivery guarantee works.subscription cursors: describes structure a subscription batch cursor.committing cursors: to send offset positions a partition nakadi storage.checking current position: to determine current offsets a subscription.subscription statistics: viewing metrics a subscription.deleting subscription: to remove subscription.getting listing subscriptions: to view individual subscription list existing susbcriptions. a more detailed description advanced configuration options please a at nakadi swagger file.creating subscriptions subscription be created posting the /subscriptions collection resource:curl -v -xpost \"http://localhost:8080/subscriptions\" -h \"content-type: application/json\" -d '{ \"owning_application\": \"order-service\", \"event_types\": [\"order.order_received\"] }' response returns whole subscription object was created, including server generated id field:http/1.1 201 createdcontent-type: application/json;charset=utf-8{ \"owning_application\": \"order-service\", \"event_types\": [ \"order.order_received\" ], \"consumer_group\": \"default\", \"read_from\": \"end\", \"id\": \"038fc871-1d2c-4e2e-aa29-1579e8f2e71f\", \"created_at\": \"2016-09-23t16:35:13.273z\"}consuming events a subscriptionconsuming events is done sending get request the subscriptions's event resource (/subscriptions/{subscription-id}/events):curl -v -xget \"http://localhost:8080/subscriptions/038fc871-1d2c-4e2e-aa29-1579e8f2e71f/events\" response is stream groups events json batches separated an endline () character. output looks this:http/1.1 200 okx-nakadi-streamid: 70779f46-950d-4e48-9fca-10c413845e7ftransfer-encoding: chunked{\"cursor\":{\"partition\":\"5\",\"offset\":\"543\",\"event_type\":\"order.order_received\",\"cursor_token\":\"b75c3102-98a4-4385-a5fd-b96f1d7872f2\"},\"events\":[{\"metadata\":{\"occurred_at\":\"1996-10-15t16:39:57+07:00\",\"eid\":\"1f5a76d8-db49-4144-ace7-e683e8ff4ba4\",\"event_type\":\"aruha-test-hila\",\"partition\":\"5\",\"received_at\":\"2016-09-30t09:19:00.525z\",\"flow_id\":\"blahbloh\"},\"data_op\":\"c\",\"data\":{\"order_number\":\"abc\",\"id\":\"111\"},\"data_type\":\"blah\"},\"info\":{\"debug\":\"stream started\"}]}{\"cursor\":{\"partition\":\"5\",\"offset\":\"544\",\"event_type\":\"order.order_received\",\"cursor_token\":\"a28568a9-1ca0-4d9f-b519-dd6dd4b7a610\"},\"events\":[{\"metadata\":{\"occurred_at\":\"1996-10-15t16:39:57+07:00\",\"eid\":\"1f5a76d8-db49-4144-ace7-e683e8ff4ba4\",\"event_type\":\"aruha-test-hila\",\"partition\":\"5\",\"received_at\":\"2016-09-30t09:19:00.741z\",\"flow_id\":\"blahbloh\"},\"data_op\":\"c\",\"data\":{\"order_number\":\"abc\",\"id\":\"111\"},\"data_type\":\"blah\"}]}{\"cursor\":{\"partition\":\"5\",\"offset\":\"545\",\"event_type\":\"order.order_received\",\"cursor_token\":\"a241c147-c186-49ad-a96e-f1e8566de738\"},\"events\":[{\"metadata\":{\"occurred_at\":\"1996-10-15t16:39:57+07:00\",\"eid\":\"1f5a76d8-db49-4144-ace7-e683e8ff4ba4\",\"event_type\":\"aruha-test-hila\",\"partition\":\"5\",\"received_at\":\"2016-09-30t09:19:00.741z\",\"flow_id\":\"blahbloh\"},\"data_op\":\"c\",\"data\":{\"order_number\":\"abc\",\"id\":\"111\"},\"data_type\":\"blah\"}]}{\"cursor\":{\"partition\":\"0\",\"offset\":\"545\",\"event_type\":\"order.order_received\",\"cursor_token\":\"bf6ee7a9-0fe5-4946-b6d6-30895baf0599\"}}{\"cursor\":{\"partition\":\"1\",\"offset\":\"545\",\"event_type\":\"order.order_received\",\"cursor_token\":\"9ed8058a-95be-4611-a33d-f862d6dc4af5\"}}each batch contains following fields:cursor: cursor the batch should used committing batch.events: array events this batch.info: optional field can hold useful information (e.g. reason why stream was closed nakadi).please note when stream is started, client receives header x-nakadi-streamid must used committing cursors. see full list parameters can used control stream events, please an api specification swagger file.client rebalancing you need more one client your subscription distribute load increase throughput - can read subscription multiple clients nakadi automatically balance load across .the balancing unit is partition, the number clients your subscription 't higher the total number all partitions the event-types your subscription. example, suppose had subscription two event-types and b, 2 4 partitions respectively. you start reading events a single client, the client get events all 6 partitions. a second client connects, 3 partitions be transferred first client a second client, resulting each client consuming 3 partitions. this case, maximum possible number clients the subscription is 6, where each client be allocated 1 partition consume. subscription api provides guarantee at-least-once delivery. practice means clients see duplicate event the case where are errors committing events. however events were successfully committed not resent. useful technique detect handle duplicate events consumer side is be idempotent to check eid field event metadata. note: eid checking is possible using \"undefined\" category, it's supplied the \"business\" \"data\" categories.subscription cursors cursors the subscription api the following structure:{ \"partition\": \"5\", \"offset\": \"543\", \"event_type\": \"order.order_received\", \"cursor_token\": \"b75c3102-98a4-4385-a5fd-b96f1d7872f2\"} fields are:partition: partition batch belongs . a batch only one partition.offset: offset this batch. offset is server defined opaque the client - clients should try infer assume structure.event_type: specifies event-type the cursor ( in stream can events different event-types);cursor_token: cursor token generated nakadi.committing cursorscursors be committed posting subscription's cursor resource (/subscriptions/{subscriptionid}/cursors), example:curl -v -xpost \"http://localhost:8080/subscriptions/038fc871-1d2c-4e2e-aa29-1579e8f2e71f/cursors\"\\ -h \"x-nakadi-streamid: ae1e39c3-219d-49a9-b444-777b4b03e84c\" \\ -h \"content-type: application/json\" \\ -d '{ \"items\": [ { \"partition\": \"0\", \"offset\": \"543\", \"event_type\": \"order.order_received\", \"cursor_token\": \"b75c3102-98a4-4385-a5fd-b96f1d7872f2\" }, { \"partition\": \"1\", \"offset\": \"923\", \"event_type\": \"order.order_received\", \"cursor_token\": \"a28568a9-1ca0-4d9f-b519-dd6dd4b7a610\" } ] }'please aware x-nakadi-streamid header is required doing commit. value should the same you in x-nakadi-streamid header opening stream events. , each client commit the batches were sent it. possible successful responses a commit are:204: cursors were successfully committed offset was increased.200: cursors were committed at least of cursors didn't increase offset it was less equal already committed . in case this response code user get json a response body a list cursors the results their commits. timeout commit is 60 seconds. you open stream, read data don't commitanything 60 seconds - stream connection be closed nakadi side. please note if are events available send you only empty batches - is need commit, nakadi close connection if is uncommitted data nocommits happened 60 seconds. the connection is closed some reason the client still has 60 seconds commit events received the moment the events were sent. that session be considered closed it be possible do commits that x-nakadi-streamid. the commit was done - the next you start reading a subscription will data the last point your commit, you again receive events haven't committed. a rebalance happens a partition is transferred another client - commit timeout 60 seconds saves day again. first client have 60 seconds do commit that partition, that partition is started stream a client. if commit wasn't done 60 seconds the streaming start a point last successful commit. other case the commit was done the client - data this partition be immediately streamed second client ( there is uncommitted data left there is need wait more). is necessary commit each batch. the cursor is committed, events are before cursor the partition also considered committed. example suppose offset was e0 the stream below,partition: [ e0 | e1 | e2 | e3 | e4 | e5 | e6 | e7 | e8 | e9 ] offset--^ the stream sent three batches the client, where client committed batch 3 not batch 1 batch 2,partition: [ e0 | e1 | e2 | e3 | e4 | e5 | e6 | e7 | e8 | e9 ] offset--^ |--- batch1 ---|--- batch2 ---|--- batch3 ---| | | | v | | [ e1 | e2 | e3 ] | | v | [ e4 | e5 | e6 ] | v [ e7 | e8 | e9 ] client: cursor commit --> |--- batch3 ---| the offset be moved the up e9 implicitly committing the events were the previous batches 1 2,partition: [ e0 | e1 | e2 | e3 | e4 | e5 | e6 | e7 | e8 | e9 ] ^-- offsetchecking current position can check current position your subscription:curl -v -xget \"http://localhost:8080/subscriptions/038fc871-1d2c-4e2e-aa29-1579e8f2e71f/cursors\" response be list current cursors reflect last committed offsets:http/1.1 200 ok{ \"items\": [ { \"partition\": \"0\", \"offset\": \"8361\", \"event_type\": \"order.order_received\", \"cursor_token\": \"35e7480a-ecd3-488a-8973-3aecd3b678ad\" }, { \"partition\": \"1\", \"offset\": \"6214\", \"event_type\": \"order.order_received\", \"cursor_token\": \"d1e5d85e-1d8d-4a22-815d-1be1c8c65c84\" } ]}subscription statistics api provides statistics your subscription:curl -v -xget \"http://localhost:8080/subscriptions/038fc871-1d2c-4e2e-aa29-1579e8f2e71f/stats\" output contain statistics all partitions the stream:http/1.1 200 ok{ \"items\": [ { \"event_type\": \"order.order_received\", \"partitions\": [ { \"partition\": \"0\", \"state\": \"reassigning\", \"unconsumed_events\": 2115, \"stream_id\": \"b75c3102-98a4-4385-a5fd-b96f1d7872f2\" }, { \"partition\": \"1\", \"state\": \"assigned\", \"unconsumed_events\": 1029, \"stream_id\": \"ae1e39c3-219d-49a9-b444-777b4b03e84c\" } ] } ]}deleting subscription delete subscription, send delete request the subscription resource using id field (/subscriptions/{id}):curl -v -x delete \"http://localhost:8080/subscriptions/038fc871-1d2c-4e2e-aa29-1579e8f2e71f\"successful response:http/1.1 204 contentgetting listing subscriptions view subscription send get request the subscription resource resource using id field (/subscriptions/{id}): :curl -v -xget \"http://localhost:8080/subscriptions/038fc871-1d2c-4e2e-aa29-1579e8f2e71f\"successful response:http/1.1 200 ok{ \"owning_application\": \"order-service\", \"event_types\": [ \"order.order_received\" ], \"consumer_group\": \"default\", \"read_from\": \"end\", \"id\": \"038fc871-1d2c-4e2e-aa29-1579e8f2e71f\", \"created_at\": \"2016-09-23t16:35:13.273z\"} get list subscriptions send get request the subscription collection resource:curl -v -xget \"http://localhost:8080/subscriptions\"example answer:http/1.1 200 ok{ \"items\": [ { \"owning_application\": \"order-service\", \"event_types\": [ \"order.order_received\" ], \"consumer_group\": \"default\", \"read_from\": \"end\", \"id\": \"038fc871-1d2c-4e2e-aa29-1579e8f2e71f\", \"created_at\": \"2016-09-23t16:35:13.273z\" } ], \"_links\": { \"next\": { \"href\": \"/subscriptions?offset=20&limit=20\" } }}'s possible filter list the following parameters: event_type, owning_application., the following pagination parameters are available: offset, limit.build developmentbuilding project is built gradle. ./gradlewwrapper script bootstrap right gradle version it's already installed. gradle setup is fairly standard, main tasks are:./gradlew build: run build test./gradlew clean: clean down build other useful tasks are:./gradlew acceptancetest: run ats./gradlew fullacceptancetest: run ats the context docker./gradlew startnakadi: build nakadi start docker-compose services: nakadi, postgresql, zookeeper kafka./gradlew stopnakadi: shutdown docker-compose services./gradlew startstorages: start docker-compose services: postgres, zookeeper kafka (useful development purposes)./gradlew stopstorages: shutdown docker-compose services working an ide, eclipse ide task is available you'll able import build.gradle intellij idea directly.dependencies nakadi server is java 8 spring boot application. uses kafka 0.9 its broker postgresql 9.5 its supporting database.nakadi requires recent versions docker docker-compose. particular, docker-compose >= v1.7.0 is required. install dockercompose information installing most recent docker-compose version. does project already implement? rest abstraction kafka- queues creation event types low-level interfacemanual client side partition management is needed support commits high-level interface (subscription api)automatic redistribution partitions between consuming clientscommits should issued move server-side cursors support event filtering per subscriptionscontributingnakadi accepts contributions the open-source community. please the issue tracker things work .before making contribution, please let know posting comment the relevant issue. if would to propose new feature, start new issue explaining feature d to contribute."
}