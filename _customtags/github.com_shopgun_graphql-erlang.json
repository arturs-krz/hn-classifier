{
	"_id": "14396310",
	"site": "https://github.com/shopgun/graphql-erlang",
	"title": "GraphQL library for Erlang",
	"author": "valberg",
	"date": "2017-06-13T13:56:41.612Z",
	"tags": {
		"categories": [
			"database", "library"
		],
		"languages": [
			"erlang",
			"other"
		]
	},
	"content": "readme.md graphql server library - erlang project contains necessary support code implement graphqlservers erlang. major is top some existingtransport library, instance cowboy web server. a requestarrives, can processed the graphql support library agraphql answer be given. a , this replaces of restendpoints a single endpoint: for graph queries. readme provides system overview its mode operation.documentation is big library. order ease development, have provided complete tutorial graphql erlang:https://github.com/shopgun/graphql-erlang-tutorial, the tutorial has book describes the tutorial exampleis implemented detail:https://shopgun.github.io/graphql-erlang-tutorial/note: read tutorial before reading in repository youhaven't already. readme gives very quick overview, thecanonical documentation is book the moment.versionversioning generally follows semantic versioning.0.8.0 - open source release. version is deliberately set bit before 1.0 order be able do changes the apibefore releasing more official version full backwardscompatibility ensured. is graphqlgraphql is query language the web. allows client tell server it wants a declarative . the server materializes response based the clients query. makes development client-centric client-driven, tend be lotfaster a development perspective. project is usually driven the top--the-iceberg down, shuffling more onus theclient side is wise move modern system design.graphql is a contract. queries responses are typed contract-verified both input output side. is, graphql acts a contract-checker. ensures: client provide illegal queries the server backend. are filtered by graphql layer. server provide illegal responses the client. arealtered such the client gets valid response according theschema replacing failing nodes null-values. contract documents api contract describes the client query data. is closest hateoas will probably without going .queries tend be large all-encompassing. means don'tpay round-trip- for request/response you in e.g.,http http/2 based systems where multiple queries are executed to and depends each . almost every query behandled a single round trip.finally, graphql supports introspection its endpoint. allowssystems query server order learn the schema is. turn, tooling be built top graphql servers providedevelopment-debug user interfaces. , languages static types use introspection derive type model code matches contract. either static code generation, by type providers.whirlwind tour graphql world specifies typed schema definition. instance following taken the relay modern specification:interface node { id: id!}type faction : node { id: id! name: string ships: shipconnection}type ship : node { id: id! name: string}type shipconnection { edges: [shipedge] pageinfo: pageinfo!}type shipedge { cursor: string! node: ship}type pageinfo { hasnextpage: boolean! haspreviouspage: boolean! startcursor: string endcursor: string}type query { rebels: faction empire: faction node(id: id!): node}input introduceshipinput { factionid: string! shipnamed: string! clientmutationid: string!}type introduceshippayload { faction: faction ship: ship clientmutationid: string!}type mutation { introduceship(input: introduceshipinput!): introduceshippayload} schema is subset the star wars schema given the typicalgraphql example over web. graphql world roughly splits world input objects output objects. input objects aregiven part a query request the client. output objects aresent from server the client. erlang implementation contains schema parser schemas the above. once parsed, mapping is provided the programmer maps output type the schema an erlang module. modulemust implement function-spec execute(context, object, field, args) -> {ok, response} | {error, reason}. is used materialize said object. is, you request field f the object o, call is made execute(context, o, f, args). value context provides globalcontext the query. is used authentication data, originip addresses so . the context is extensible the developer any field need. args provides arguments the field., for instance the type mutation the introduceshipfield, takes argument input type introduceshipinput!.materialization is thus simply function call the erlang world. calls tend be used two ways: either acquire piece data a database (e.g., mnesia) return data anobject. they materialize fields an already loaded object. execution a query is processed, can imagine having \"cursor\" is being moved around the result set is used materialize each part the query. example, at following query:query q { node(id: \"12098141\") { ... ship { id name } }} this query executes, will start a developer providedinitial object. typically empty map #{}. since node fieldis requested, call is performed match:-module(query)....execute(ctx, #{}, <<\"node\">>, #{ <<\"id\">> := id }) -> {ok, obj} = load_object(id)., since are requesting id name fields a shipinside node, system make callback a type-resolver the obj order determine type is. omit parthere, if was something else, faction , then rest the query not trigger. once know id \"12098141\" is ship, \"move cursor\" a ship calls execute function:-module(ship).-record(ship, { id, name }).execute(ctx, #ship{ id = id }, <<\"id\">>, _args) -> {ok, id};execute(ctx, #ship{ name = name }, <<\"name\">>, _args) -> {ok, name}. materialization calls be made. for field <<\"id\">> one the field <<\"name\">>. end result is materialized a response the caller.materilization through derivation common of functions is derive data existing data.suppose extend ship the following :type ship { ... capacity : float! load : float! loadratio : float!} a ship has certain capacity a current load its cargo bay. could store loadratio the mnesia database keep up date. a more efficient to handle is compute from data:-module(ship).-record(ship, { id, name, capacity, load }).execute(...) -> ...;execute(ctx, #ship { capacity = cap, load = load }, <<\"loadratio\">>, _) -> {ok, load / cap };... will compute field it is requested, not compute when is requested a client. many fields a data set arederivable this fashion. especially a schema changes grows time. old fields be derived backwards compatibility new fields be added next it. addition, tends be more efficient. sizable portion modern web is moving data around. you to move lessdata, decrease memory network pressure, cantranslate faster service.materializing joins we a at faction type, see following:type faction : node { id: id! name: string ships: shipconnection} this, ships is field referring a shipconnection. connection type is relay modern standard how handle paginated set objects graphql. \"materialization derivation\" would derive field looking the data thedatabase the join then producing object theship_connection_resource handle. instance:execute(ctx, #faction { id = id }, <<\"ships\">>, _args) -> {ok, ships} = ship:lookup_by_faction(id), pagination:build_pagination(ships).where build_pagination function returns object is generic connection object. will probably something along lines #{ '$type' => <<\"shipconnection\">>, <<\"pageinfo\">> => #{ <<\"hasnextpage\">> => false, ... }, <<\"edges\">> => [ #{ <<\"cursor\">> => base64:encode(<<\"edge:1\">>), <<\"node\">> => #ship{ ... } }, ...]} can be processed further other resources. note weare eagerly constructing several objects once then exploiting cursor moves the graphql system materialize fields theclient requests. alternative is lazily constructmaterializations demand, when data is readily available anyway, is often more efficient just pass pointers along.api graphql api is defined the module graphql. everyfunctionality is exported that module. not call inside modules their functionality change any point time between major releases. system deliberately splits each phase hands over theprogrammer. allows to debug bit easier gives programmer more control the parts. typical implementation start using schema loader:inject() -> {ok, file} = application:get_env(myapp, schema_file), priv = code:priv_dir(myapp), fname = filename:join([priv, file]), {ok, schemadata} = file:read_file(fname), map = #{ scalars => #{ default => scalar_resource }, interfaces => #{ default => resolve_resource }, unions => #{ default => resolve_resource }, objects => #{ 'ship' => ship_resource, 'faction' => faction_resource, ... 'query' => query_resource, 'mutation' => mutation_resource } }, ok = graphql:load_schema(map, schemadata), root = {root, #{ query => 'query', mutation => 'mutation', interfaces => [] }}, ok = graphql:insert_schema_definition(root), ok = graphql:validate_schema(), ok. will set the schema the code reading from file disk. each the _resource names refers modules implements backend code. order execute queries the schema, code such the following be used. have query document doc we a requestedoperation name opname parameter variables the given op vars. variables req state are standard cowboy request state tracking variables cowboy_rest.run(doc, opname, vars, req, state) -> case graphql:parse(doc) {ok, ast} -> try elaborated = graphql:elaborate(ast), {ok, #{fun_env := funenv, ast := ast2 }} = graphql:type_check(elaborated), ok = graphql:validate(ast2), coerced = graphql:type_check_params(funenv, opname, vars), ctx = #{ params => coerced, operation_name => opname }, response = graphql:execute(ctx, ast2), req2 = cowboy_req:set_resp_body(encode_json(response), req), {ok, reply} = cowboy_req:reply(200, req2), {halt, reply, state} catch throw:err -> err(400, err, req, state) end; {error, error} -> err(400, {parser_error, error}, req, state) end.conventions this graphql implementation, default value keys are typebinary(). choice is deliberate, since makes code moreresistent atom() overflow also avoids conversionsbetween binary() atom() values the system. later version the library might redesign aspect, we are somewhat stuck it now.however, are many places where can input atom values then them converted internally the library binary values. greatly simplifies large number data entry tasks theprogrammer. general rules are: you supply value the system it is atom(), internal representation is binary value. the system hands a value, is binary() value not atom().middlewares graphql system does support middlewares, it turns the systems design is flexible enough middlewares be implemented developers themselves. observation is any query runsthrough query type thus query_resource. likewise, mutation factors through mutation_resource. a result, can implement middlewares using execute/4function a wrapper. instance could define mutationfunction :execute(ctx, obj, field, args) -> annotctx = perform_authentication(ctx), execute_field(annotctx, obj, field, args). reason works well is we are able use patternmatching execute/4 functions then specialize . if had individual function each field, we have been forced implement middlewares the system, incurs more code lines support.more complex systems define stack middlewares the list run one one. an example, clientmutationid is part the relay modern specification must present everymutation. can build mutation_resource such it runs maps:/2 the argument input, runs underlying mutation, then adds the clientmutationid afterwards.schema extensions graphql implementation loosely follows apollo extensionmechanism. plan adapt whatever default is eventually chosen the graphql later. can annotate specification additional tagging writing +tag(args) where args aretraditional arguments graphql data. tags are available the context you execute fields. special tags exist:+description(text: string!) - used write -line documentation an element the graphql schema. is possible write multi-line comment through use backticks rather doublequotes. graphql accepts markdown a number description blocks the backtick blocks -line preformatted sections, iswhy what chosen. an example, can write something along lines :+description(text: \" ship the star wars universe\") type ship : node { +description(text: \"unique identity the ship\") id: id! +description(text: \" descriptive name the ship\") name: string } the schema parser knows to transform into documentation.resource modules following section documents layout resource modules theyare used graphql, what are needed in implementation.scalar resourcesgraphql contains major kinds data: objects scalars. objectsare product types where each element the product is field. rawdata are represented scalar values. graphql defines number standard scalar values: boolean, integers, floating point numbers,enumerations, strings, identifiers so . but can extend set scalars yourself. spec contain something along lines scalar colorscalar datetime so . these are mapped onto resource modules handling scalars. is often enough provide default scalar module the mapping then implement functions handle scalars:-module(scalar_resource).-export( [input/2, output/2]).-spec input(type, value) -> {ok, coerced} | {error, reason} type :: binary(), value :: binary(), coerced :: (), reason :: term().input(<<\"color\">>, c) -> color:coerce(c);input(<<\"datetime\">>, dt) -> datetime:coerce(dt);input(ty, v) -> error_logger:info_report({coercing_generic_scalar, ty, v}), {ok, v}.-spec output(type, value) -> {ok, coerced} | {error, reason} type :: binary(), value :: binary(), coerced :: (), reason :: term().output(<<\"color\">>, c) -> color:as_binary(c);output(<<\"datetime\">>, dt) -> datetime:as_binary(dt);output(ty, v) -> error_logger:info_report({output_generic_scalar, ty, v}), {ok, v}.scalar mappings allow to an internal externalrepresentation values. could instance read color such #aabbcc, convert into #{ r => 0.66, g => 0.73, b => 0.8 }internally back again outputting . likewise datetimeobject be converted a unix timestamp a timezone internally you . you also handle multiple different ways coercing input data, have multiple internal data representations.type resolution resources graphql function correctly, must able resolve types concrete objects. is the graphql system allows tospecify abstract interfaces unions. example the aboveschema is node interface is implemented ship faction among things. we are trying materialize node, graphql must a to figure the type the object ismaterializing. is handled the type resolution mapping:-module(resolve_resource).-export([execute/1]).%% following is probably included a header file a real%% implementation-record(ship, {id, name}).-record(faction, {id, name}).execute(#ship{}) -> {ok, <<\"ship\">>};execute(#faction{}) -> {ok, <<\"faction\">>};execute(obj) -> {error, unknown_type}.output object resourceseach (output) object is mapped onto erlang module responsible handling field requests that object. module looks :-module(object_resource).-export([execute/4]).execute(ctx, srcobj, <<\"f\">>, args) -> {ok, 42};execute(ctx, srcobj, field, args) -> default only function is needed is execute/4 function iscalled the system whenever field is requested that object. 4 parameters are follows:ctx - context the query. contains informationpertaining the current position the graph, well user-supplied information the start the request. iscommonly used a read- store authentication/authorizationdata, you limit certain users see.srcobj - current object which are operating. imagine have ships, b-wing an x-wing. if request same fields the ships, srcobj is going be different.graphql often proceeds having certain fields fetch objects of backing store then moving cursor onto object calling correct object resource that type. srcobj isset point the object is currently being operated upon.field - field the object is requested.args - map field arguments. the next section.field argument rules graphql, field arguments follow specific pattern:clients has way input null value. only thing can is omit given field the input. particular, clientsmust supply field is non-null.servers always every field the input, if clientdoesn't supply . if client does supply field, it has default value, server sees null value that field. pattern means is clear for client specify \"value\" a clear for server work the case where client specified \" value. eliminates corner cases where have figure what client meant.resolution follows rather simple pattern graphql. a clientomits field it has default value, default value is input.otherwise null is input. clients must supply every non-null field. the server side, handle arguments supplying map kv pairs the execute function. suppose have input such input point { x = 4.0 float y float} server handle input matching directly:execute(ctx, srcobj, field, #{ <<\"x\">> := xval, <<\"y\">> := yval }) -> ... will always match. the client provides input {} is empty input, xval be 4.0 due the default value. yval be null. the client supplies, e.g., { x: 2.0, y: 7.0 } map #{ <<\"x\">> => 2.0, <<\"y\">> => 7.0 } be provided.tips & tricks execute function allows to object-level generic handling fields. , for example, srcobj is map, can genericlookups using following handler:execute(_ctx, obj, field, _args) -> case maps:(field, obj, not_found) not_found -> {ok, null}; val -> {ok, val} end. this is very common, graphql system currently supplies shorthand this:execute(_ctx, _obj, _field, _args) -> default.note: shorthand may removed a future major version itturns it be handled quite easily the programmer ageneric . the assumption srcobj is map() type is limitation doesn't necessarily hold true. your databasebackend is mnesia, is likely be record instance.another trick is use generic execution handle \"middlewares\" - the appropriate section middlewares.system architecture other graphql servers provide type->module mapping. rather, rely binding individual functions fields. implementation began the same setup, it turns patternmatching is good fit the notion requesting different fieldsinside object. thus, use pattern matching a destructuringmechanism incoming queries.schemainternally, system parses schema an ets table, which can perform queries parallel satisfy multiple requests thesame .a schema injector allows developer parse schema a file from memory, bind exeuction modules the schemas types. oncefinishes, schema is finalized runs lint check theschema rejects schemas are nonsensical.query query is treated a compiler chain, is design fitserlang . compilers rely lot pattern matching, we process query symbolically matching it graduallytransforming into query plan can be executed. lexer tokenizes query parser constructs ast the query the token stream elaborator walks ast attaches type information theast looking data the schema ets table. vastlysimplifies later phases the necessary information is oftendirectly available a pattern match. elaborator performs early exit obviously malformed queries. type checker validates query a type perspective. validator performs additional linting. many queries aretype-correct thus executable, are still malformed they nonsensical parts them. validator phase rejectssuch queries. query plan is formed. executor runs query plan. these tasks, the execution phase the end isperformance-critical. clients pre-load query documents theserver, means document acts a stored procedure theserver side. server then parsing, elaboration, typechecking validation once for at load . in addition provides security measure: clients production only call pre-validated set queries such desired.tips & tricksgraphql has very neat javascript tooling plugs theintrospection a graphql server provides additionalfunctionality:graphiql - provides query interface autocompletion,documentation, debugging, ways execute queries so . it ishighly recommended add such system staging production it is indispensable you are trying figure a query why given query returned specific kind error.additionally, relay modern provides specifications cacherefreshing, pagination, mutation specifications so . it isrecommended implement those parts your system it is part a de-facto standard how graphql servers tend operate.statuscurrently, code implements of october 2016 graphqlspecification, except a few areas: validators are missing pending implementation. important validators are present, however.parametrization inside fragments are yet implemented fully.parallelization is postponed until refactoring phase has beencompleted the code base. is fairly plan itsimplementation present, we've had need implement parallel behavior yet. system still needs specification sending to workerprocesses through system based \"promises\". allows tocoalesce data loading, database joins dynamically speeds queries. code is somewhat rough places needs refactoring.tests graphql project has extensive test suite. prefer addingregressions the suite we experience . some the tests aretaken the official graphql repository translated. more is definitely needed, in general functionality should provided together a test case demonstrates newfunctionality. have dungeon schema loosely reflects mud schema use a game. is used test regressions the graphqlspecification, to test breakage backwards compatibility."
}