{
	"_id": "14420653",
	"site": "https://github.com/joeddav/devol",
	"title": "DEvol: automated deep neural network design via genetic programming",
	"author": "timanglade",
	"date": "2017-06-13T13:56:32.587Z",
	"tags": {
		"categories": [
			"data-science"
		],
		"languages": [
			"python",
			"jupyter notebook"
		]
	},
	"content": "readme.md devol - deep neural network evolutiondevol (deepevolution) utilizes genetic programming automatically architect deep neural network optimal hyperparameters a given dataset using keras library. approach should design equal superior model what human design working under same constraints are imposed upon genetic program (e.g., maximum number layers, maximum number convolutional filters per layer, etc.). current setup is designed classification problems, though could extended include other output type well. demo.ipynb a simple example.evolutioneach model is represented fixed-width genome encoding information the network's structure. the current setup, model contains number convolutional layers, number dense layers, an optimizer. convolutional layers be evolved include varying numbers feature maps, different activation functions, varying proportions dropout, whether perform batch normalization /or max pooling. same options are available the dense layers the exception max pooling. complexity these models easily extended beyond capabilities include parameters included keras, allowing creation more complex architectures.below is highly simplified visualization how genetic crossover might place between models.genetic crossover mutation neural networksresults demonstration, ran program the mnist dataset ( demo.ipynb an example setup) 20 generations a population size 50. allowed model to 6 convolutional layers 4 dense layers (including softmax layer). best accuracy attained 10 epochs training under constraints was 99.4%, is higher we were able achieve manually constructing own models under same constraints. graphic below displays running maximum accuracy all 1000 nets they evolve 20 generations.keep mind these results are obtained simple, relatively shallow neural networks no data augmentation, transfer learning, ensembling, fine-tuning, other optimization techniques. however, virtually of methods be incorporated the genetic program.running max mnist accuracies across 20 generationsapplication most significant barrier using devol a real problem is complexity the algorithm. training neural networks is often such computationally expensive process, training hundreds thousands different models evaluate fitness each is always feasible. below are approaches combat issue:parallel training - nature evaluating fitness multiple members a population simultaneously is embarassingly parallel. task this be trivial distribute among many gpus even machines.early stopping - 's need train model 10 epochs it stops improving 3; cut off early.train fewer epochs - training a genetic program serves purpose: evaluate model's fitness relation other models. may be necessary train convergence make comparison; may need 2 3 epochs. however, is important exercise caution decreasing training because doing could create evolutionary pressure toward simpler models converge quickly. creates trade-off between training and accuracy , depending the application, may may be desirable.parameter selection - more robust allow models be, longer will to converge; .e., don't allow horizontal flipping a character recognition problem though genetic program eventually learn to include . the less space program has explore, faster will arrive an optimal solution. some problems, may ideal simply plug data devol let program build complete model you, for others, hands-off approach may be feasible. either case, devol give insights optimal model design you may have considered your own. the mnist digit classification problem, found relu does far better a sigmoid function convolutional layers, they about equally in dense layers. also found adagrad was highest-performing prebuilt optimizer gained insight the number nodes include each dense layer. worst, devol give insight improving model architecture. best, could you beautiful, finely-tuned model.wanna try ?to setup, clone repo run pip install -e path//repo. should be able access devol globally.devol is pretty straightforward use basic classification problems. demo.ipynb an example. are three basic steps:prep dataset. devol expects classification problem labels are -hot encoded it uses categorical_crossentropy its loss function. otherwise, can prep data however 'd . just pass input shape genomehandler.create genomehandler. genomehandler defines constraints you apply your models. specify maximum number convolutional dense layers, max dense nodes feature maps, the input shape. can specify whether 'd to allow batch_normalization, dropout, max_pooling, are included default. can pass a list optimizers activation functions 'd to allow.create run devol. pass genomehandler the devol constructor, run . here have few more options such the number generations, population size, epochs used fitness evaluation, evaluation metric optimize (accuracy loss) an (optional) fitness function converts model's accuracy loss a fitness score. demo.ipynb a basic example."
}